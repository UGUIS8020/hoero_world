from __future__ import annotations
import base64, json, time, logging, traceback, os
from flask import render_template, request, current_app, jsonify
from boto3.dynamodb.conditions import Key
from botocore.exceptions import ClientError
from urllib.parse import quote_plus
import feedparser
from hashlib import sha256 as _sha
from . import bp
import requests
import xml.etree.ElementTree as ET
from datetime import datetime

logger = logging.getLogger(__name__)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# ========= å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _d(msg: str):
    """DEBUGå‡ºåŠ›ï¼ˆFlask DEBUGæ™‚ã¯å¿…ãšå‡ºã™ï¼‰"""
    try:
        if current_app and current_app.debug:
            print(msg)
            logger.info(msg)
        else:
            logger.info(msg)
    except Exception:
        print(msg)

def _iso_now_utc():
    """ç¾åœ¨æ™‚åˆ»ã‚’ISOå½¢å¼ã§å–å¾—"""
    from datetime import datetime, timezone
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def _ensure_iso(v):
    """DynamoDB ãŒ datetime ã‚’å—ã‘å–ã‚Œãªã„ãŸã‚ ISO æ–‡å­—åˆ—ã«æƒãˆã‚‹"""
    from datetime import datetime, date, timezone
    if v is None:
        return None
    if isinstance(v, datetime):
        if v.tzinfo is None:
            v = v.replace(tzinfo=timezone.utc)
        else:
            v = v.astimezone(timezone.utc)
        return v.strftime("%Y-%m-%dT%H:%M:%SZ")
    if isinstance(v, date):
        return datetime(v.year, v.month, v.day, tzinfo=timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    return str(v)

def _dynamodb_sanitize(v):
    """DynamoDB ã«æ¸¡ã™è¾æ›¸ã‚’å®‰å…¨åŒ–ï¼ˆdatetimeâ†’ISO æ–‡å­—åˆ— ãªã©ï¼‰"""
    from datetime import datetime, date, timezone
    if isinstance(v, datetime):
        if v.tzinfo is None:
            v = v.replace(tzinfo=timezone.utc)
        else:
            v = v.astimezone(timezone.utc)
        return v.strftime("%Y-%m-%dT%H:%M:%SZ")
    if isinstance(v, date):
        return datetime(v.year, v.month, v.day, tzinfo=timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    if isinstance(v, dict):
        return {k: _dynamodb_sanitize(v2) for k, v2 in v.items()}
    if isinstance(v, (list, tuple)):
        return type(v)(_dynamodb_sanitize(x) for x in v)
    if isinstance(v, set):
        return list(_dynamodb_sanitize(x) for x in v)
    return v

# ========= DynamoDB I/O =========
def _table():
    return current_app.config["DENTAL_TABLE"]

def _hash_url(url: str) -> str:
    return _sha(url.encode()).hexdigest()

def put_unique_dental(item: dict) -> bool:
    """æ­¯ç§‘ãƒ‹ãƒ¥ãƒ¼ã‚¹å°‚ç”¨ã®DynamoDBä¿å­˜é–¢æ•°ï¼ˆAIå¯¾å¿œç‰ˆï¼‰"""
    pk = f"URL#{_hash_url(item['url'])}"
    try:
        _table().put_item(
            Item=_dynamodb_sanitize({
                "pk": pk, "sk": "METADATA",
                "url": item["url"], 
                "title": item.get("title"),
                "source": item.get("source"), 
                "kind": item.get("kind"),
                "lang": item.get("lang"), 
                "published_at": (_ensure_iso(item.get("published_at")) or _iso_now_utc()),
                "summary": item.get("summary"), 
                "image_url": item.get("image_url"),
                "author": item.get("author"),
                "gsi1pk": f"KIND#{item.get('kind')}#LANG#{item.get('lang')}",
                "gsi1sk": _ensure_iso(item.get("published_at")) or "0000-00-00T00:00:00",
                # AIåˆ¤å®šçµæœã‚’ä¿å­˜
                "ai_relevant": item.get("ai_relevant"),
                "ai_kind": item.get("ai_kind"),
                "ai_summary": item.get("ai_summary"),
                "ai_reason": item.get("ai_reason"),
                "ai_search_query": item.get("ai_search_query"),
                "ai_headline": item.get("ai_headline"),
            }),
            ConditionExpression="attribute_not_exists(pk)"
        )
        _d(f"[DB] âœ… Saved: {item.get('title', '')[:60]}")  # â˜… æˆåŠŸãƒ­ã‚°
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "ConditionalCheckFailedException":
            _d(f"[DB] âš ï¸ Already exists: {item.get('title', '')[:60]}")  # â˜… é‡è¤‡ãƒ­ã‚°
            return False
        _d(f"[DB] âŒ Error saving: {e}")  # â˜… ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        raise
    except Exception as e:
        _d(f"[DB] âŒ Unexpected error: {e}")  # â˜… ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
        traceback.print_exc()
        raise

def dental_query_items(kind="research", lang="ja", limit=40, last_evaluated_key=None):
    """æ­¯ç§‘ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ã‚¯ã‚¨ãƒªé–¢æ•°"""
    kwargs = {
        "IndexName": "gsi1",
        "KeyConditionExpression": Key("gsi1pk").eq(f"KIND#{kind}#LANG#{lang}"),
        "ScanIndexForward": False,
        "Limit": limit,
    }
    if last_evaluated_key:
        kwargs["ExclusiveStartKey"] = last_evaluated_key
    resp = _table().query(**kwargs)
    return resp.get("Items", []), resp.get("LastEvaluatedKey")

# ========= AIåé›†æ©Ÿèƒ½ =========
def ai_filter_and_classify(title: str, summary: str | None, lang: str = "ja") -> dict:
    """AIã§è¨˜äº‹ã®é–¢é€£åº¦åˆ¤å®šã¨åˆ†é¡ï¼ˆOpenAIç‰ˆï¼‰"""
    prompt = f"""ä»¥ä¸‹ã®è¨˜äº‹ãŒã€Œè‡ªå®¶æ­¯ç‰™ç§»æ¤ï¼ˆtooth autotransplantationï¼‰ã€ã«é–¢é€£ã™ã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹æƒ…å ±ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {title}
è¦ç´„: {summary or "ãªã—"}

ã€åˆ¤å®šåŸºæº–ã€‘
é–¢é€£ã™ã‚‹å†…å®¹ï¼š
- è‡ªå®¶æ­¯ç‰™ç§»æ¤ã€æ­¯ç‰™ç§»æ¤ã€æ­¯ã®ç§»æ¤ã«é–¢ã™ã‚‹æŠ€è¡“ãƒ»ç ”ç©¶ãƒ»ç—‡ä¾‹
- ãƒ‰ãƒŠãƒ¼ãƒ¬ãƒ—ãƒªã‚«ã€3Dãƒ—ãƒªãƒ³ãƒˆã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãªã©ã®é–¢é€£æŠ€è¡“
- ç§»æ¤ç”¨ã®è£½å“ãƒ»åŒ»ç™‚æ©Ÿå™¨ï¼ˆã‚¢ãƒ«ãƒ™ã‚ªã‚·ã‚§ãƒ¼ãƒãƒ¼ãªã©ï¼‰
- è¦ªçŸ¥ã‚‰ãšã‚„ä½™å‰°æ­¯ã‚’ä½¿ã£ãŸç§»æ¤ç—‡ä¾‹
- å‰æ­¯ã¸ã®ç§»æ¤ãªã©å…·ä½“çš„ãªç—‡ä¾‹å ±å‘Š
- ã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆã¨ã®æ¯”è¼ƒè¨˜äº‹

é™¤å¤–ã™ã‚‹å†…å®¹ï¼š
- çœ¼ç§‘ã€æ•´å½¢å¤–ç§‘ã€ç¾å®¹å¤–ç§‘ãªã©æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªåˆ†é‡
- è‡“å™¨ç§»æ¤ãªã©æ­¯ç§‘ä»¥å¤–ã®ç§»æ¤

ã€è¨˜äº‹åˆ†é¡ã€‘
- research: ç ”ç©¶ãƒ»ä¸€èˆ¬è¨˜äº‹
- case: ç—‡ä¾‹å ±å‘Š
- video: å‹•ç”»ãƒ»ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«  
- product: è£½å“æƒ…å ±ãƒ»åŒ»ç™‚æ©Ÿå™¨
- market: å¸‚å ´ãƒ¬ãƒãƒ¼ãƒˆãƒ»çµ±è¨ˆ

ã€å‡ºåŠ›è¦ä»¶ã€‘
- ai_summary ã¨ headline_ja ã¯å¿…ãš**æ—¥æœ¬èª**ã§æ›¸ã„ã¦ãã ã•ã„
- headline_ja ã¯ãŠãŠã‚ˆã20æ–‡å­—ä»¥å†…ã®çŸ­ã„è¦‹å‡ºã—ã¨ã—ã€ä½“è¨€æ­¢ã‚ã‚’æ¨å¥¨ã—ã¾ã™
  ä¾‹: ã€Œ3Dãƒ¬ãƒ—ãƒªã‚«ã‚’ç”¨ã„ãŸç§»æ¤è¡“ã€ã€Œè¦ªçŸ¥ã‚‰ãšç§»æ¤ã®é•·æœŸäºˆå¾Œã€ãªã©

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
  "relevant": true/false,
  "kind": "research/case/video/product/market",
  "ai_summary": "è¨˜äº‹ã®è¦ç‚¹ã‚’40ã€œ80å­—ç¨‹åº¦ã®æ—¥æœ¬èª1æ–‡ã§è¦ç´„ã€‚å¯èƒ½ã§ã‚ã‚Œã°ã€ã©ã®ã‚ˆã†ãªæ‚£è€…ãƒ»æ­¯ã€ã€ã©ã‚“ãªæ–¹æ³•ï¼ˆ3Dãƒ—ãƒªãƒ³ãƒˆã‚„ã‚¬ã‚¤ãƒ‰æ‰‹è¡“ãªã©ï¼‰ã€ã€ã©ã‚“ãªçµæœãƒ»æ„ç¾©ï¼ˆé•·æœŸäºˆå¾Œã‚„å¯©ç¾æ€§ã®æ”¹å–„ãªã©ï¼‰ã€ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã®è¨€ã„æ›ãˆã ã‘ã®çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«ã¯ã—ãªã„ã§ãã ã•ã„ã€‚",
  "headline_ja": "30ã€œ45æ–‡å­—ç¨‹åº¦ã®è‡ªç„¶ãªæ—¥æœ¬èªã®è¦‹å‡ºã—ã€‚åè©ã®ç¾…åˆ—ã§ã¯ãªãã€ã€ã€œã‚’å ±å‘Šã€ã€ã€œãŒç¤ºã•ã‚ŒãŸã€ã€ã€œã«ã‚ˆã‚Šæ”¹å–„ã—ãŸã€ã€ã€œã®ç—‡ä¾‹ã€ãªã©ã®è¡¨ç¾ã‚’å«ã‚€æ–‡ç« èª¿ã«ã—ã¦ãã ã•ã„ã€‚åŸé¡Œã®ç›´è¨³ã§ã¯ãªãã€æ—¥æœ¬ã®æ­¯ç§‘ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆé¢¨ã®èª­ã¿ã‚„ã™ã„è¦‹å‡ºã—ã«ã—ã¦ãã ã•ã„ã€‚",
  "reason": "åˆ¤å®šç†ç”±ã‚’ç°¡æ½”ã«ï¼ˆæ—¥æœ¬èªï¼‰"
}}

DO NOT OUTPUT ANYTHING OTHER THAN VALID JSON."""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {OPENAI_API_KEY}"
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1000,
                "temperature": 0.3
            },
            timeout=30
        )
        
        if response.status_code != 200:
            _d(f"[AI] API error: {response.status_code} - {response.text}")
            return {
                "relevant": False,
                "kind": "research",
                "ai_summary": "",
                "ai_headline": "",
                "reason": "API error",
            }
        
        data = response.json()
        
        if "choices" not in data or len(data["choices"]) == 0:
            _d(f"[AI] Unexpected response: {json.dumps(data, indent=2)}")
            return {
                "relevant": False,
                "kind": "research",
                "ai_summary": "",
                "ai_headline": "",
                "reason": "Unexpected API response",
            }
        
        result_text = data["choices"][0]["message"]["content"].strip()
        result_text = result_text.replace("```json", "").replace("```", "").strip()
        result = json.loads(result_text)
        
        return {
            "relevant": result.get("relevant", False),
            "kind": result.get("kind", "research"),
            "ai_summary": result.get("ai_summary", ""),
            "ai_headline": result.get("headline_ja", ""),  # â˜… ã“ã“ã§å¤‰æ›
            "reason": result.get("reason", "")
        }
        
    except Exception as e:
        _d(f"[AI] Filter error: {e}")
        traceback.print_exc()
        return {
            "relevant": False,
            "kind": "research",
            "ai_summary": "",
            "ai_headline": "",
            "reason": f"Error: {e}",
        }

def ai_collect_news(lang="ja", max_iterations=5):
    """AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªå¾‹çš„ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»è«–æ–‡ã‚’åé›†ï¼ˆGoogle News + PubMedï¼‰"""
    collected_urls = set()
    all_items = []
    search_history = []
    
    # å…±é€šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    base_context = """ã‚ãªãŸã¯è‡ªå®¶æ­¯ç‰™ç§»æ¤ï¼ˆtooth autotransplantationï¼‰ã«é–¢ã™ã‚‹
æœ€æ–°æƒ…å ±ã‚’åé›†ã™ã‚‹å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®è¦³ç‚¹ã§å¹…åºƒãæƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š
- æŠ€è¡“é©æ–°ï¼ˆ3Dãƒ—ãƒªãƒ³ãƒˆã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€CAD/CAMï¼‰
- æ–°è£½å“ãƒ»åŒ»ç™‚æ©Ÿå™¨ï¼ˆã‚¢ãƒ«ãƒ™ã‚ªã‚·ã‚§ãƒ¼ãƒãƒ¼ã€ãƒ¬ãƒ—ãƒªã‚«ã‚·ã‚¹ãƒ†ãƒ ãªã©ï¼‰
- è‡¨åºŠç—‡ä¾‹ï¼ˆç‰¹ã«å‰æ­¯ã¸ã®ç§»æ¤ã€ä¸Šé¡ä¸­åˆ‡æ­¯ã€è¦ªçŸ¥ã‚‰ãšã®æ´»ç”¨ãªã©ï¼‰
- ç ”ç©¶è«–æ–‡ï¼ˆæˆåŠŸç‡ã€é•·æœŸäºˆå¾Œã€PDLä¿å­˜ãªã©ï¼‰
- å¸‚å ´å‹•å‘ãƒ»çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
- æ¯”è¼ƒè¨˜äº‹ï¼ˆã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆ vs è‡ªå®¶æ­¯ç‰™ç§»æ¤ãªã©ï¼‰"""

    # â˜…æ—¥æœ¬èªã ã‘ã‚¯ã‚¨ãƒªã‚’ã€Œã‚†ã‚‹ãã€ã™ã‚‹è¿½åŠ æŒ‡ç¤º
    if lang == "ja":
        context = base_context + """

ã€é‡è¦ï¼šæ—¥æœ¬èªæ¤œç´¢ç”¨ã®æ³¨æ„ã€‘
- æ—¥æœ¬èªè¨˜äº‹ã¯ãƒ’ãƒƒãƒˆãŒå°‘ãªã„ã®ã§ã€ã‚¯ã‚¨ãƒªã¯ 2ã€œ3 èªç¨‹åº¦ã«ã—ã¦ãã ã•ã„ã€‚
- å¿…ãšã€Œè‡ªå®¶æ­¯ç‰™ç§»æ¤ã€ã€Œæ­¯ç‰™è‡ªå®¶ç§»æ¤ã€ã€Œæ­¯ã®è‡ªå®¶ç§»æ¤ã€ã„ãšã‚Œã‹ã®åŸºæœ¬èªã‚’å«ã‚ã€
  ãã‚Œã« 1 èªã ã‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¶³ã™ç¨‹åº¦ã«ã—ã¦ãã ã•ã„ã€‚
  ä¾‹: "è‡ªå®¶æ­¯ç‰™ç§»æ¤ ç—‡ä¾‹", "è‡ªå®¶æ­¯ç‰™ç§»æ¤ äºˆå¾Œ", "æ­¯ã®è‡ªå®¶ç§»æ¤ ç ”ç©¶"
- ã€Œ3Dãƒ—ãƒªãƒ³ãƒˆã€ã€ŒCAD/CAMã€ã€Œã‚¢ãƒ«ãƒ™ã‚ªã‚·ã‚§ãƒ¼ãƒãƒ¼ã€ãªã©ãƒ‹ãƒƒãƒãªèªã¯ã€
  å…¨ä½“ã® 1ã€œ2 ã‚¯ã‚¨ãƒªã«ã¨ã©ã‚ã¦ãã ã•ã„ã€‚
"""
    else:
        context = base_context

    # ===== ãƒ¡ã‚¤ãƒ³ã®è‡ªå¾‹æ¤œç´¢ãƒ«ãƒ¼ãƒ— =====
    for iteration in range(max_iterations):
        query_prompt = f"""{context}

ã€ã“ã‚Œã¾ã§ã®æ¤œç´¢å±¥æ­´ã€‘
{json.dumps(search_history, ensure_ascii=False, indent=2) if search_history else "ã¾ã æ¤œç´¢ã—ã¦ã„ã¾ã›ã‚“"}

ã€åé›†æ¸ˆã¿è¨˜äº‹æ•°ã€‘{len(all_items)}ä»¶

æ¬¡ã«å®Ÿè¡Œã™ã¹ãæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
- æ—¢å­˜ã®æ¤œç´¢ã¨é‡è¤‡ã—ãªã„æ–°ã—ã„åˆ‡ã‚Šå£ã§æ¢ã—ã¦ãã ã•ã„
- {lang}è¨€èªï¼ˆ{'æ—¥æœ¬èª' if lang == 'ja' else 'è‹±èª'}ï¼‰ã§ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
- å…·ä½“çš„ãªè£½å“åã€æŠ€è¡“åã€ç—‡ä¾‹ã‚¿ã‚¤ãƒ—ãªã©ã‚’å«ã‚ã¦ãã ã•ã„
- **æ¤œç´¢ã‚¯ã‚¨ãƒªã¯ã€Œæ–‡ç« ã€ã§ã¯ãªãæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³å‘ã‘ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã«ã—ã¦ãã ã•ã„**
  ï¼ˆä¾‹: æ—¥æœ¬èªãªã‚‰ "è‡ªå®¶æ­¯ç‰™ç§»æ¤ ç—‡ä¾‹",
       è‹±èªãªã‚‰ "tooth autotransplantation 3D printed replica"ï¼‰
- ã“ã‚Œã‚‰ã®ã‚¯ã‚¨ãƒªã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‚„è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆGoogle News / PubMed ãªã©ï¼‰ã§åˆ©ç”¨ã•ã‚Œã¾ã™

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
  "queries": [
    {{"query": "æ¤œç´¢ã‚¯ã‚¨ãƒª1", "reason": "ãªãœã“ã®æ¤œç´¢ãŒå¿…è¦ã‹"}},
    {{"query": "æ¤œç´¢ã‚¯ã‚¨ãƒª2", "reason": "ãªãœã“ã®æ¤œç´¢ãŒå¿…è¦ã‹"}},
    {{"query": "æ¤œç´¢ã‚¯ã‚¨ãƒª3", "reason": "ãªãœã“ã®æ¤œç´¢ãŒå¿…è¦ã‹"}}
  ],
  "strategy": "ä»Šå›ã®æ¤œç´¢æˆ¦ç•¥ã®èª¬æ˜"
}}

DO NOT OUTPUT ANYTHING OTHER THAN VALID JSON."""
        try:
            # OpenAI API ã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {OPENAI_API_KEY}"
                },
                json={
                    "model": "gpt-4o-mini",
                    "messages": [{"role": "user", "content": query_prompt}],
                    "max_tokens": 2000,
                    "temperature": 0.7
                },
                timeout=30
            )
            
            _d(f"[AI AGENT] API status code: {response.status_code}")
            
            if response.status_code != 200:
                _d(f"[AI AGENT] API error response: {response.text}")
                continue
            
            data = response.json()
            
            if "choices" not in data or len(data["choices"]) == 0:
                _d(f"[AI AGENT] Unexpected response format: {json.dumps(data, indent=2)}")
                continue
                
            result_text = data["choices"][0]["message"]["content"].strip()
            result_text = result_text.replace("```json", "").replace("```", "").strip()
            query_plan = json.loads(result_text)
            
            _d(f"[AI AGENT] Iteration {iteration+1}: {query_plan['strategy']}")
            
            for q_item in query_plan["queries"]:
                query = q_item["query"]
                reason = q_item["reason"]
                
                _d(f"[AI AGENT] Searching: {query}")
                
                search_results = []

                # â‘  Google News
                search_results.extend(_execute_google_search(query, lang))

                # â‘¡ PubMedï¼ˆè‹±èªã®ã¿ï¼‰
                if lang == "en":
                    pubmed_results = _execute_pubmed_search(query, max_results=20)
                    search_results.extend(pubmed_results)
                
                for result_item in search_results:
                    if not result_item.get("url"):
                        continue
                    if result_item["url"] in collected_urls:
                        continue
                    
                    ai_result = ai_filter_and_classify(
                        result_item["title"], 
                        result_item.get("summary"), 
                        lang
                    )
                    
                    if ai_result["relevant"]:
                        result_item["lang"] = lang
                        result_item["kind"] = ai_result["kind"]
                        result_item["ai_relevant"] = ai_result["relevant"]
                        result_item["ai_kind"] = ai_result["kind"]
                        result_item["ai_summary"] = ai_result["ai_summary"]
                        result_item["ai_reason"] = ai_result["reason"]
                        result_item["ai_search_query"] = query
                        result_item["ai_headline"] = ai_result.get("ai_headline")
                        
                        all_items.append(result_item)
                        collected_urls.add(result_item["url"])
                        
                        _d(
                            f"[AI AGENT] âœ“ Found: {result_item['title'][:60]}..."
                            f" (kind={ai_result['kind']}, lang={lang})"
                        )
                
                search_history.append({
                    "iteration": iteration + 1,
                    "query": query,
                    "reason": reason,
                    "found": len(search_results)
                })
                
                time.sleep(1)
                
        except Exception as e:
            _d(f"[AI AGENT] Error in iteration {iteration+1}: {e}")
            traceback.print_exc()

    # ===== ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ—¥æœ¬èªã§1ä»¶ã‚‚æ‹¾ãˆã¦ã„ãªã„å ´åˆ =====
    if lang == "ja" and not all_items:
        fallback_queries = [
            "è‡ªå®¶æ­¯ç‰™ç§»æ¤ ç—‡ä¾‹",
            "è‡ªå®¶æ­¯ç‰™ç§»æ¤ äºˆå¾Œ èª¿æŸ»",
            "æ­¯ã®è‡ªå®¶ç§»æ¤ ç ”ç©¶"
        ]
        for query in fallback_queries:
            _d(f"[AI AGENT] Fallback searching (ja): {query}")
            search_results = _execute_google_search(query, "ja")

            for result_item in search_results:
                if not result_item.get("url"):
                    continue
                if result_item["url"] in collected_urls:
                    continue

                ai_result = ai_filter_and_classify(
                    result_item["title"],
                    result_item.get("summary"),
                    "ja"
                )

                if not ai_result["relevant"]:
                    continue

                result_item["lang"] = "ja"
                result_item["kind"] = ai_result["kind"]
                result_item["ai_relevant"] = ai_result["relevant"]
                result_item["ai_kind"] = ai_result["kind"]
                result_item["ai_summary"] = ai_result["ai_summary"]
                result_item["ai_reason"] = ai_result["reason"]
                result_item["ai_search_query"] = query
                result_item["ai_headline"] = ai_result.get("ai_headline")

                all_items.append(result_item)
                collected_urls.add(result_item["url"])
                _d(
                    f"[AI AGENT] âœ“ Fallback Found: {result_item['title'][:60]}..."
                    f" (kind={ai_result['kind']}, lang=ja)"
                )

        # ãƒ­ã‚°ç”¨ã«å±¥æ­´ã‚‚è¿½åŠ ã—ã¦ãŠã
        search_history.append({
            "iteration": "fallback",
            "query": " / ".join(fallback_queries),
            "reason": "æ—¥æœ¬èªãƒ¢ãƒ¼ãƒ‰ã§0ä»¶ã ã£ãŸãŸã‚å›ºå®šã‚¯ã‚¨ãƒªã§å†æ¤œç´¢",
            "found": len(all_items)
        })
    
    # ===== DynamoDB ã¸ä¿å­˜ =====
    saved = 0
    for item in all_items:
        if put_unique_dental(item):
            saved += 1
            _d(f"[AI AGENT] ğŸ’¾ Saved: {item['title'][:60]}...")
    
    _d(f"[AI AGENT] âœ… Complete: total_found={len(all_items)}, saved={saved}")
    
    return {
        "total_found": len(all_items),
        "saved": saved,
        "search_history": search_history
    }

def _execute_google_search(query, lang="ja"):
    """å®Ÿéš›ã®Google Newsæ¤œç´¢ã‚’å®Ÿè¡Œ"""
    if lang == "ja":
        hl, gl, ceid = "ja", "JP", "JP:ja"
    else:
        hl, gl, ceid = "en", "US", "US:en"
    
    url = f"https://news.google.com/rss/search?q={quote_plus(query)}&hl={hl}&gl={gl}&ceid={ceid}"
    
    feed = feedparser.parse(url)
    items = []
    
    for e in feed.entries:
        link = getattr(e, "link", None)
        if not link:
            continue
            
        title = (getattr(e, "title", "") or "").strip()
        summary = getattr(e, "summary", None)
        
        pub = getattr(e, "published", None) or getattr(e, "updated", None)
        published_at = _iso_now_utc()
        if pub and getattr(e, "published_parsed", None):
            try:
                import datetime
                import time as _time
                tm = e.published_parsed
                published_at = datetime.datetime.utcfromtimestamp(
                    _time.mktime(tm)
                ).strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                pass
        
        items.append({
            "source": "google_news_ai",
            "title": title,
            "url": link,
            "published_at": published_at,
            "summary": summary,
            "author": getattr(getattr(e, "source", None) or {}, "title", None),
            "image_url": None,
            "lang": lang,
        })
    
    return items


def _execute_pubmed_search(query: str, max_results: int = 20):
    """PubMed ã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—ã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã™"""
    try:
        # 1. ID ãƒªã‚¹ãƒˆã‚’å–å¾—
        r = requests.get(
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
            params={
                "db": "pubmed",
                "term": query,
                "retmode": "json",
                "retmax": max_results,
                "sort": "pub+date",   # ç™ºè¡Œæ—¥ã®æ–°ã—ã„é †
            },
            timeout=10,
        )
        r.raise_for_status()
        data = r.json()
        ids = data.get("esearchresult", {}).get("idlist", [])
        if not ids:
            return []

        id_str = ",".join(ids)

        # 2. è©³ç´°æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãªã©ï¼‰ã‚’ XML ã§å–å¾—
        r2 = requests.get(
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
            params={
                "db": "pubmed",
                "id": id_str,
                "retmode": "xml",
            },
            timeout=20,
        )
        r2.raise_for_status()
        root = ET.fromstring(r2.text)

        def _parse_pubdate(pubdate_elem):
            """PubDate è¦ç´ ã‹ã‚‰ ISO æ–‡å­—åˆ—ã‚’ã§ãã‚‹ç¯„å›²ã§ä½œã‚‹"""
            if pubdate_elem is None:
                return _iso_now_utc()

            year = pubdate_elem.findtext("Year")
            month = pubdate_elem.findtext("Month") or "01"
            day = pubdate_elem.findtext("Day") or "01"

            # æœˆãŒ "Jan" ãªã©ã®çœç•¥è¡¨è¨˜ã®å ´åˆã«å¯¾å¿œ
            month_map = {
                "Jan": "01", "Feb": "02", "Mar": "03", "Apr": "04",
                "May": "05", "Jun": "06", "Jul": "07", "Aug": "08",
                "Sep": "09", "Oct": "10", "Nov": "11", "Dec": "12",
            }
            month = month_map.get(month, month)

            try:
                dt = datetime(int(year), int(month), int(day))
                return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                return _iso_now_utc()

        items = []

        for art in root.findall(".//PubmedArticle"):
            pmid_el = art.find(".//PMID")
            pmid = pmid_el.text if pmid_el is not None else None

            article = art.find(".//Article")
            title = ""
            if article is not None:
                title_el = article.find("ArticleTitle")
                if title_el is not None:
                    # ã‚¿ã‚°ã‚’å«ã‚€ã“ã¨ãŒã‚ã‚‹ã®ã§ itertext ã§çµåˆ
                    title = "".join(title_el.itertext()).strip()

            abstract = ""
            abstr_el = article.find("Abstract") if article is not None else None
            if abstr_el is not None:
                parts = []
                for t in abstr_el.findall("AbstractText"):
                    parts.append("".join(t.itertext()).strip())
                abstract = " ".join(parts)

            journal = ""
            journal_el = article.find("Journal/Title") if article is not None else None
            if journal_el is not None:
                journal = journal_el.text

            pubdate_el = art.find(".//PubDate")
            published_at = _parse_pubdate(pubdate_elem=pubdate_el)

            url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else None

            items.append({
                "source": "pubmed",
                "title": title,
                "url": url,
                "published_at": published_at,
                "summary": abstract,
                "author": journal,   # or first author ã§ã‚‚OK
                "image_url": None,
                "lang": "en",        # PubMed ã¯åŸºæœ¬è‹±èªæ‰±ã„
            })

        return items

    except Exception as e:
        _d(f"[PUBMED] Error: {e}")
        traceback.print_exc()
        return []

# ========= ã‚µãƒ¼ãƒ“ã‚¹ =========
def _enc_tok(lek: dict | None) -> str | None:
    if not lek: return None
    return base64.urlsafe_b64encode(json.dumps(lek).encode()).decode()

def _dec_tok(tok: str | None):
    if not tok: return None
    try:
        return json.loads(base64.urlsafe_b64decode(tok.encode()).decode())
    except Exception:
        return None

def list_news(kind="research", lang="ja", limit=40, tok=None):
    lek = _dec_tok(tok)
    items, next_lek = dental_query_items(kind=kind, lang=lang, limit=limit, last_evaluated_key=lek)
    return items, _enc_tok(next_lek)

# ========= ãƒ«ãƒ¼ãƒˆ =========
@bp.route("/news")
def news():
    return render_template("pages/news.html")

@bp.route("/autotransplant_news")
def autotransplant_news():
    kind = request.args.get("kind", "research")
    lang = request.args.get("lang", "ja")
    tok  = request.args.get("tok")
    rows, next_tok = list_news(kind=kind, lang=lang, limit=40, tok=tok)
    return render_template("pages/autotransplant_news.html",
                           rows=rows, kind=kind, lang=lang, page=1, next_tok=next_tok)

@bp.route("/api/latest")
def news_api_latest():
    kind = request.args.get("kind", "research")
    lang = request.args.get("lang", "ja")
    limit = min(int(request.args.get("limit", 5)), 20)

    # â˜… ã“ã“ã‹ã‚‰è¿½åŠ ï¼šlang=all ã®ã¨ãã¯ ja + en ã‚’ã¾ã¨ã‚ã¦è¿”ã™
    if lang == "all":
        combined = []
        for lg in ["ja", "en"]:
            items, _ = dental_query_items(kind=kind, lang=lg, limit=limit)
            combined.extend(items)

        # published_at ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        combined.sort(key=lambda x: x.get("published_at", ""), reverse=True)

        # URLã§é‡è¤‡æ’é™¤ã—ã¤ã¤ã€æœ€å¤§ limit ä»¶ã¾ã§
        seen = set()
        payload = []
        for it in combined:
            url = it.get("url")
            if not url or url in seen:
                continue
            seen.add(url)

            # â˜… è¦‹å‡ºã—ã®å„ªå…ˆé †ä½: ai_headline > ai_summary > title
            headline = (
                it.get("ai_headline")
                or it.get("ai_summary")
                or it.get("title")
            )

            payload.append({
                "title": headline,
                "url": url,
                "published_at": (it.get("published_at") or "")[:10],
                "kind": it.get("kind"),
                "lang": it.get("lang"),
                "source": it.get("source"),
            })

            if len(payload) >= limit:
                break

        return jsonify({
            "kind": kind, "lang": "all",
            "count": len(payload),
            "updated_at": _iso_now_utc(),
            "items": payload,
        })

    # â˜… ã“ã“ã‹ã‚‰ä¸‹ã¯ä»Šã¾ã§ã®ã¾ã¾ï¼ˆlang ãŒ ja / en ã®ã¨ãï¼‰
    items, _ = dental_query_items(kind=kind, lang=lang, limit=limit, last_evaluated_key=None)
    payload = [
        {
            "title": it.get("title"),
            "url": it.get("url"),
            "published_at": (it.get("published_at") or "")[:10],
            "kind": it.get("kind"),
        }
        for it in items
        if it.get("title") and it.get("url")
    ]
    return jsonify({
        "kind": kind, "lang": lang,
        "count": len(payload),
        "updated_at": _iso_now_utc(),
        "items": payload
    })


@bp.route("/api/all")
def news_api_all():
    """å…¨ã¦ã®è¨˜äº‹ã‚’å–å¾—ï¼ˆå…¨ç¨®é¡ãƒ»å…¨è¨€èªï¼‰"""
    all_items = []
    
    # å…¨ç¨®é¡ãƒ»å…¨è¨€èªã‚’å–å¾—
    kinds = ["research", "case", "video", "product", "market"]
    langs = ["ja", "en"]
    
    for kind in kinds:
        for lang in langs:
            items, _ = dental_query_items(kind=kind, lang=lang, limit=100)
            all_items.extend(items)
    
    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    all_items.sort(key=lambda x: x.get("published_at", ""), reverse=True)
    
    # é‡è¤‡å‰Šé™¤ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
    seen_urls = set()
    unique_items = []
    for item in all_items:
        if item["url"] not in seen_urls:
            seen_urls.add(item["url"])
            unique_items.append({
                "title": item.get("title"),
                "url": item.get("url"),
                "published_at": (item.get("published_at") or "")[:10],
                "kind": item.get("kind"),
                "lang": item.get("lang"),
                "ai_summary": item.get("ai_summary"),
                "author": item.get("author"),
                "image_url": item.get("image_url"),
            })
    
    return jsonify({
        "count": len(unique_items),
        "updated_at": _iso_now_utc(),
        "items": unique_items
    })


@bp.route("/admin/inspect")
def news_admin_inspect():
    kinds = ["research", "case", "video", "product", "market"]
    langs = ["ja", "en"]
    lines = []
    for k in kinds:
        for lg in langs:
            items, _ = dental_query_items(kind=k, lang=lg, limit=5)
            titles = [i.get("title") for i in items]
            lines.append(f"{k}/{lg}: count~{len(items)} sample={titles}")
    _d("[INSPECT] " + " | ".join(lines))
    return "<br>".join(lines)

@bp.route("/admin/debug_dump")
def news_admin_debug_dump():
    items, _ = dental_query_items(kind="research", lang="ja", limit=20)
    html = ["<h3>research/ja (top 20)</h3><ol>"]
    for it in items:
        html.append(f"<li>{it.get('published_at','')} â€” {it.get('title','')}<br>"
                    f"<small>{it.get('url','')}</small></li>")
    html.append("</ol>")
    return "".join(html)


@bp.route("/admin/run_autotransplant_news")
def run_autotransplant_news():
    """AIåé›†ã‚’å®Ÿè¡Œï¼ˆæ—¢å­˜ã®ãƒ«ãƒ¼ãƒˆã‹ã‚‰å‘¼ã³å‡ºã—ï¼‰"""
    try:
        # AIåé›†ã‚’å®Ÿè¡Œ
        results_ja = ai_collect_news(lang="ja", max_iterations=5)
        time.sleep(2)
        results_en = ai_collect_news(lang="en", max_iterations=3)
        
        total = results_ja["saved"] + results_en["saved"]
        
        # æœ€åˆã®è¨˜äº‹ã‚’ç¢ºèª
        items, _ = dental_query_items(kind="research", lang="ja", limit=1)
        
        _d(f"[DEBUG] after collect: total={total}, ja={results_ja['saved']}, en={results_en['saved']}")
        
        return f"""
        <h1>AIåé›†å®Œäº†ï¼</h1>
        <p>åˆè¨ˆ: {total}ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ</p>
        <ul>
            <li>æ—¥æœ¬èª: {results_ja['saved']}ä»¶ï¼ˆæ¤œç´¢{len(results_ja['search_history'])}å›ï¼‰</li>
            <li>è‹±èª: {results_en['saved']}ä»¶ï¼ˆæ¤œç´¢{len(results_en['search_history'])}å›ï¼‰</li>
        </ul>
        <p>æœ€åˆã®è¨˜äº‹: {'ã‚ã‚Š' if items else 'ãªã—'}</p>
        <h3>æ¤œç´¢å±¥æ­´ï¼ˆæ—¥æœ¬èªï¼‰:</h3>
        <pre>{json.dumps(results_ja['search_history'], ensure_ascii=False, indent=2)}</pre>
        <h3>æ¤œç´¢å±¥æ­´ï¼ˆè‹±èªï¼‰:</h3>
        <pre>{json.dumps(results_en['search_history'], ensure_ascii=False, indent=2)}</pre>
        <p><a href="/news/autotransplant_news?kind=research&lang=ja">æ—¥æœ¬èªè¨˜äº‹ã‚’è¦‹ã‚‹</a></p>
        <p><a href="/news/autotransplant_news?kind=research&lang=en">è‹±èªè¨˜äº‹ã‚’è¦‹ã‚‹</a></p>
        """
    except Exception as e:
        traceback.print_exc()
        return f"<h1>ã‚¨ãƒ©ãƒ¼</h1><pre>{traceback.format_exc()}</pre>"
    

@bp.route("/admin/clear_all_dental_news")
def clear_all_dental_news():
    """å…¨è¨˜äº‹ã‚’å‰Šé™¤ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    try:
        # å…¨è¨˜äº‹ã‚’å–å¾—ã—ã¦å‰Šé™¤
        table = _table()
        deleted = 0
        
        # å…¨ç¨®é¡ãƒ»å…¨è¨€èªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        for kind in ["research", "case", "video", "product", "market"]:
            for lang in ["ja", "en"]:
                lek = None
                while True:
                    items, next_lek = dental_query_items(kind=kind, lang=lang, limit=100, last_evaluated_key=lek)
                    
                    for item in items:
                        table.delete_item(Key={"pk": item["pk"], "sk": item["sk"]})
                        deleted += 1
                    
                    if not next_lek:
                        break
                    lek = next_lek
        
        return f"å‰Šé™¤å®Œäº†: {deleted}ä»¶ã®è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"
    




