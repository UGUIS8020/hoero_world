from __future__ import annotations
import base64, json, time, logging, traceback, os
from flask import render_template, request, current_app, jsonify
from botocore.exceptions import ClientError
from urllib.parse import quote_plus
import feedparser
from hashlib import sha256 as _sha
from . import bp
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from boto3.dynamodb.conditions import Attr
from flask_login import current_user, login_required
from urllib.parse import quote


@bp.route('/admin/delete_article')
@login_required
def admin_delete_article():
    """ç‰¹å®šè¨˜äº‹ã‚’å‰Šé™¤ï¼ˆç®¡ç†è€…ã®ã¿ï¼‰"""
    if not current_user.is_administrator:
        return "æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“", 403
    
    url = request.args.get('url')
    
    if not url:
        return "URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™", 400
    
    confirm = request.args.get('confirm')
    
    table = _table()
    pk = f"URL#{_hash_url(url)}"
    
    try:
        response = table.get_item(Key={"pk": pk, "sk": "METADATA"})
        item = response.get('Item')
        
        if not item:
            return f"è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}", 404
        
        # ç¢ºèªç”»é¢
        if not confirm:
            # â˜… URLã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦æ¸¡ã™
            encoded_url = quote(url, safe='')
            
            html = f"""
            <h1>è¨˜äº‹å‰Šé™¤ã®ç¢ºèª</h1>
            <div style="border: 2px solid red; padding: 20px; margin: 20px 0;">
                <h2>ä»¥ä¸‹ã®è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ</h2>
                <p><strong>Title:</strong> {item.get('title')}</p>
                <p><strong>URL:</strong> {item.get('url')}</p>
                <p><strong>Kind:</strong> {item.get('kind')} | <strong>Lang:</strong> {item.get('lang')}</p>
                <p><strong>Published:</strong> {item.get('published_at')}</p>
            </div>
            <p>
                <a href="/news/admin/delete_article?url={encoded_url}&confirm=yes" 
                   style="background: red; color: white; padding: 10px 20px; text-decoration: none;">
                   å‰Šé™¤ã™ã‚‹
                </a>
                <a href="/news/autotransplant_news" 
                   style="background: gray; color: white; padding: 10px 20px; text-decoration: none; margin-left: 10px;">
                   ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                </a>
            </p>
            """
            return html
        
        # å®Ÿéš›ã«å‰Šé™¤
        table.delete_item(Key={"pk": pk, "sk": "METADATA"})
        
        return f"""
        <h1>å‰Šé™¤å®Œäº†</h1>
        <p>è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {item.get('title')}</p>
        <p><a href="/news/autotransplant_news">ä¸€è¦§ã«æˆ»ã‚‹</a></p>
        """
        
    except Exception as e:
        import traceback
        return f"<pre>{traceback.format_exc()}</pre>", 500
    

logger = logging.getLogger(__name__)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# ========= å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _d(msg: str):
    """DEBUGå‡ºåŠ›ï¼ˆFlask DEBUGæ™‚ã¯å¿…ãšå‡ºã™ï¼‰"""
    try:
        if current_app and current_app.debug:
            print(msg)
            logger.info(msg)
        else:
            logger.info(msg)
    except Exception:
        print(msg)

def _iso_now_utc():
    """ç¾åœ¨æ™‚åˆ»ã‚’ISOå½¢å¼ã§å–å¾—"""
    from datetime import datetime, timezone
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def _ensure_iso(v):
    """DynamoDB ãŒ datetime ã‚’å—ã‘å–ã‚Œãªã„ãŸã‚ ISO æ–‡å­—åˆ—ã«æƒãˆã‚‹"""
    from datetime import datetime, date, timezone
    if v is None:
        return None
    if isinstance(v, datetime):
        if v.tzinfo is None:
            v = v.replace(tzinfo=timezone.utc)
        else:
            v = v.astimezone(timezone.utc)
        return v.strftime("%Y-%m-%dT%H:%M:%SZ")
    if isinstance(v, date):
        return datetime(v.year, v.month, v.day, tzinfo=timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    return str(v)

def _dynamodb_sanitize(v):
    """DynamoDB ã«æ¸¡ã™è¾æ›¸ã‚’å®‰å…¨åŒ–ï¼ˆdatetimeâ†’ISO æ–‡å­—åˆ— ãªã©ï¼‰"""
    from datetime import datetime, date, timezone
    if isinstance(v, datetime):
        if v.tzinfo is None:
            v = v.replace(tzinfo=timezone.utc)
        else:
            v = v.astimezone(timezone.utc)
        return v.strftime("%Y-%m-%dT%H:%M:%SZ")
    if isinstance(v, date):
        return datetime(v.year, v.month, v.day, tzinfo=timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    if isinstance(v, dict):
        return {k: _dynamodb_sanitize(v2) for k, v2 in v.items()}
    if isinstance(v, (list, tuple)):
        return type(v)(_dynamodb_sanitize(x) for x in v)
    if isinstance(v, set):
        return list(_dynamodb_sanitize(x) for x in v)
    return v

# ========= DynamoDB I/O =========
def _table():
    return current_app.config["DENTAL_TABLE"]

def _hash_url(url: str) -> str:
    return _sha(url.encode()).hexdigest()

def ai_filter_and_classify(title: str, summary: str = None, lang: str = "ja", url: str = None):
    """AIã‚’ä½¿ã£ã¦è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼†åˆ†é¡ã—ã€ã‚µãƒãƒªãƒ¼ã¨è¦‹å‡ºã—ã‚’ç”Ÿæˆ"""
    
    prompt = f"""
ä»¥ä¸‹ã®è¨˜äº‹ãŒã€Œè‡ªå®¶æ­¯ç‰™ç§»æ¤ï¼ˆtooth autotransplantationï¼‰ã€ã«é–¢é€£ã™ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€
é–¢é€£ã™ã‚‹å ´åˆã¯é­…åŠ›çš„ãªè¦‹å‡ºã—ã¨è¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}
è¦ç´„: {summary or "ãªã—"}
è¨€èª: {lang}
URL: {url or "ãªã—"}

ã€åˆ¤å®šåŸºæº–ã€‘
âœ… é–¢é€£ã™ã‚‹ï¼ˆrelevant: trueï¼‰ï¼š
- è‡ªå®¶æ­¯ç‰™ç§»æ¤ã®æ‰‹è¡“ã€æŠ€è¡“ã€ç—‡ä¾‹
- ç§»æ¤æ­¯ã®äºˆå¾Œã€æˆåŠŸç‡ã€ç”Ÿå­˜ç‡
- ç§»æ¤ã«ä½¿ç”¨ã™ã‚‹æ­¯ï¼ˆè¦ªçŸ¥ã‚‰ãšã€å°è‡¼æ­¯ãªã©ï¼‰
- ç§»æ¤æ™‚ã®æ­¯æ ¹è†œï¼ˆPDLï¼‰ä¿å­˜æŠ€è¡“
- ç§»æ¤å¾Œã®éª¨ãƒ»æ­¯å‘¨çµ„ç¹”å†ç”Ÿ
- 3Dãƒ—ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚°ã€CADã‚’ä½¿ã£ãŸç§»æ¤ç”¨ãƒ¬ãƒ—ãƒªã‚«
- ç§»æ¤æ­¯ã®å›ºå®šæ–¹æ³•ã€ãƒªã‚°ãƒ­ã‚¹ãªã©ã®ä½µç”¨ç™‚æ³•

âŒ é–¢é€£ã—ãªã„ï¼ˆrelevant: falseï¼‰ï¼š
- **è‡ªå®¶æ­¯ç‰™ç§»æ¤ãŒä¸»é¡Œã¨ã—ã¦æ˜ç¢ºã«å«ã¾ã‚Œã¦ã„ãªã„ä¸€èˆ¬çš„ãªæ­¯ç§‘ç ”ç©¶**
- è‚‰èŠ½çµ„ç¹”ã€å‰µå‚·æ²»ç™’ã€éª¨å†ç”Ÿãªã©ãŒä¸»é¡Œã ãŒç§»æ¤ã¨ã®é–¢é€£ãŒä¸æ˜ç¢º
- ã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆã€ãƒ–ãƒªãƒƒã‚¸ã€ç¾©æ­¯ãªã©ã®ä»–ã®æ²»ç™‚æ³•ã®ã¿
- ç ”ç©¶è€…ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã€å¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- æ­¯ç§‘åŒ»é™¢ã®æ–™é‡‘è¡¨ã€è¨ºç™‚æ¡ˆå†…
- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ï¼ˆ/archives/tag/ã€/archives/category/ï¼‰
- ã‚¿ã‚°ãƒšãƒ¼ã‚¸ã€ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
- è«–æ–‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ãƒšãƒ¼ã‚¸ï¼ˆCiNiiã€researchmapç­‰

ã€é‡è¦ã€‘
- ã‚¿ã‚¤ãƒˆãƒ«ã‚„è¦ç´„ã«ã€Œautotransplantationã€ã€Œç§»æ¤ã€ã€Œtransplantã€ãªã©ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€
  ãã‚ŒãŒ**æ­¯ã®ç§»æ¤**ã§ã¯ãªãä»–ã®åŒ»ç™‚åˆ†é‡ã®ç§»æ¤ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- **è‡ªå®¶æ­¯ç‰™ç§»æ¤ã¨ã®ç›´æ¥çš„ãªé–¢é€£æ€§**ã‚’æ…é‡ã«ç¢ºèªã—ã¦ãã ã•ã„
- ç–‘ã‚ã—ã„å ´åˆã¯ relevant: false ã¨ã—ã¦ãã ã•ã„

ã€åˆ†é¡åŸºæº–ã€‘ï¼ˆrelevantãŒtrueã®å ´åˆã®ã¿ï¼‰
- research: å­¦è¡“è«–æ–‡ã€ç ”ç©¶å ±å‘Š
- case: ç—‡ä¾‹å ±å‘Šã€æ²»ç™‚ä¾‹
- news: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã€ä¸€èˆ¬å‘ã‘æƒ…å ±
- video: å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

ã€è¦‹å‡ºã—ã¨è¦ç´„ã®ç”Ÿæˆã€‘ï¼ˆrelevantãŒtrueã®å ´åˆã®ã¿ï¼‰
- ai_headline: é­…åŠ›çš„ã§ç°¡æ½”ãªè¦‹å‡ºã—ï¼ˆ30æ–‡å­—ä»¥å†…ã€{'æ—¥æœ¬èª' if lang == 'ja' else 'è‹±èª'}ã§ï¼‰
- ai_summary: è¨˜äº‹ã®è¦ç‚¹ã‚’ã¾ã¨ã‚ãŸè¦ç´„ï¼ˆ100-150æ–‡å­—ã€{'æ—¥æœ¬èª' if lang == 'ja' else 'è‹±èª'}ã§ï¼‰

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆrelevant ãŒ false ã®å ´åˆã€ai_headline ã¨ ai_summary ã¯ç©ºæ–‡å­—åˆ—ã§æ§‹ã„ã¾ã›ã‚“ï¼‰ï¼š
{{
  "relevant": true/false,
  "kind": "research/case/news/video",
  "reason": "åˆ¤å®šç†ç”±",
  "ai_headline": "é­…åŠ›çš„ãªè¦‹å‡ºã—",
  "ai_summary": "è¨˜äº‹ã®è¦ç´„"
}}

DO NOT OUTPUT ANYTHING OTHER THAN VALID JSON.
"""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {OPENAI_API_KEY}"
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1500,  # ã‚µãƒãƒªãƒ¼ç”Ÿæˆã®ãŸã‚å¢—ã‚„ã™
                "temperature": 0.3
            },
            timeout=30
        )
        
        if response.status_code != 200:
            _d(f"[AI] API error: {response.status_code} - {response.text}")
            return {
                "relevant": False,
                "kind": "research",
                "ai_summary": "",
                "ai_headline": "",
                "reason": "API error",
            }
        
        data = response.json()
        
        if "choices" not in data or len(data["choices"]) == 0:
            _d(f"[AI] Unexpected response: {json.dumps(data, indent=2)}")
            return {
                "relevant": False,
                "kind": "research",
                "ai_summary": "",
                "ai_headline": "",
                "reason": "Unexpected API response",
            }
        
        result_text = data["choices"][0]["message"]["content"].strip()
        result_text = result_text.replace("```json", "").replace("```", "").strip()
        
        try:
            result = json.loads(result_text)
        except json.JSONDecodeError as e:
            _d(f"[AI] JSON parse error: {e}")
            _d(f"[AI] Raw response: {result_text}")
            return {
                "relevant": False,
                "kind": "research",
                "ai_summary": "",
                "ai_headline": "",
                "reason": f"JSON parse error: {e}",
            }
        
        return {
            "relevant": result.get("relevant", False),
            "kind": result.get("kind", "research"),
            "ai_summary": result.get("ai_summary", ""),
            "ai_headline": result.get("ai_headline", ""),
            "reason": result.get("reason", "")
        }
        
    except Exception as e:
        _d(f"[AI] Filter error: {e}")
        traceback.print_exc()
        return {
            "relevant": False,
            "kind": "research",
            "ai_summary": "",
            "ai_headline": "",
            "reason": f"Error: {e}",
        }

def ai_collect_news(lang="ja", max_iterations=5):
    """AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªå¾‹çš„ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»è«–æ–‡ã‚’åé›†ï¼ˆGoogle News + PubMedï¼‰"""

    # â˜… é™¤å¤–ã™ã‚‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã“ã“ã«è¿½åŠ ï¼‰
    EXCLUDED_PATTERNS = [
        '/archives/tag/',           # ç¢ºå®Ÿã«ä¸è¦
        '/archives/category/',      # ç¢ºå®Ÿã«ä¸è¦
        '/author/',                 # ç¢ºå®Ÿã«ä¸è¦ï¼ˆè‘—è€…ãƒšãƒ¼ã‚¸ï¼‰
        '/feed',                    # ç¢ºå®Ÿã«ä¸è¦ï¼ˆRSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰
        '?share=',                  # ç¢ºå®Ÿã«ä¸è¦ï¼ˆã‚·ã‚§ã‚¢ãƒªãƒ³ã‚¯ï¼‰
        '/wp-admin',               # ç¢ºå®Ÿã«ä¸è¦ï¼ˆç®¡ç†ç”»é¢ï¼‰
        '/wp-login',               # ç¢ºå®Ÿã«ä¸è¦ï¼ˆãƒ­ã‚°ã‚¤ãƒ³ï¼‰
        '/archives/tag/',
        '/archives/category/',
        '/department/',
        '/about/',
    ]
    
    # â˜… æœ€åˆã«DynamoDBã‹ã‚‰æ—¢å­˜URLã‚’èª­ã¿è¾¼ã‚“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    collected_urls = _load_existing_urls_from_db()
    _d(f"[CACHE] Starting with {len(collected_urls)} existing URLs in cache")
    
    all_items = []
    search_history = []
    
    # â˜… Google Search APIå‘¼ã³å‡ºã—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    google_api_call_count = 0
    MAX_GOOGLE_API_CALLS = 10
    
    # â˜… Google Search APIå‘¼ã³å‡ºã—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    google_api_call_count = 0
    MAX_GOOGLE_API_CALLS = 10  # 1å›ã®åé›†ã§æœ€å¤§10å›ã¾ã§
    
    # å…±é€šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    base_context = """ã‚ãªãŸã¯è‡ªå®¶æ­¯ç‰™ç§»æ¤ï¼ˆtooth autotransplantationï¼‰ã«é–¢ã™ã‚‹
æœ€æ–°æƒ…å ±ã‚’åé›†ã™ã‚‹å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®è¦³ç‚¹ã§å¹…åºƒãæƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š
- æŠ€è¡“é©æ–°ï¼ˆ3Dãƒ—ãƒªãƒ³ãƒˆã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€CAD/CAMï¼‰
- æ–°è£½å“ãƒ»åŒ»ç™‚æ©Ÿå™¨ï¼ˆã‚¢ãƒ«ãƒ™ã‚ªã‚·ã‚§ãƒ¼ãƒãƒ¼ã€ãƒ¬ãƒ—ãƒªã‚«ã‚·ã‚¹ãƒ†ãƒ ãªã©ï¼‰
- è‡¨åºŠç—‡ä¾‹ï¼ˆç‰¹ã«å‰æ­¯ã¸ã®ç§»æ¤ã€ä¸Šé¡ä¸­åˆ‡æ­¯ã€è¦ªçŸ¥ã‚‰ãšã®æ´»ç”¨ãªã©ï¼‰
- ç ”ç©¶è«–æ–‡ï¼ˆæˆåŠŸç‡ã€é•·æœŸäºˆå¾Œã€PDLä¿å­˜ãªã©ï¼‰
- å¸‚å ´å‹•å‘ãƒ»çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
- æ¯”è¼ƒè¨˜äº‹ï¼ˆã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆ vs è‡ªå®¶æ­¯ç‰™ç§»æ¤ãªã©ï¼‰
- **æµ·å¤–ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆä¸­å›½ã€éŸ“å›½ã€æ¬§ç±³ãªã©ã®æ—¥æœ¬èªè¨˜äº‹ï¼‰**  # â† è¿½åŠ 
"""

    # â˜…æ—¥æœ¬èªã ã‘ã‚¯ã‚¨ãƒªã‚’ã€Œã‚†ã‚‹ãã€ã™ã‚‹è¿½åŠ æŒ‡ç¤º
    if lang == "ja":
        context = base_context + """

ã€é‡è¦ï¼šæ—¥æœ¬èªæ¤œç´¢ç”¨ã®æ³¨æ„ã€‘
- æ—¥æœ¬èªè¨˜äº‹ã¯ãƒ’ãƒƒãƒˆãŒå°‘ãªã„ã®ã§ã€ã‚¯ã‚¨ãƒªã¯ 2ã€œ3 èªç¨‹åº¦ã«ã—ã¦ãã ã•ã„ã€‚
- å¿…ãšã€Œè‡ªå®¶æ­¯ç‰™ç§»æ¤ã€ã€Œæ­¯ç‰™è‡ªå®¶ç§»æ¤ã€ã€Œæ­¯ã®è‡ªå®¶ç§»æ¤ã€ã„ãšã‚Œã‹ã®åŸºæœ¬èªã‚’å«ã‚ã€
  ãã‚Œã« 1 èªã ã‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¶³ã™ç¨‹åº¦ã«ã—ã¦ãã ã•ã„ã€‚
  ä¾‹: "è‡ªå®¶æ­¯ç‰™ç§»æ¤ ç—‡ä¾‹", "è‡ªå®¶æ­¯ç‰™ç§»æ¤ äºˆå¾Œ", "æ­¯ã®è‡ªå®¶ç§»æ¤ ç ”ç©¶"
- ã€Œ3Dãƒ—ãƒªãƒ³ãƒˆã€ã€ŒCAD/CAMã€ã€Œã‚¢ãƒ«ãƒ™ã‚ªã‚·ã‚§ãƒ¼ãƒãƒ¼ã€ãªã©ãƒ‹ãƒƒãƒãªèªã¯ã€
  å…¨ä½“ã® 1ã€œ2 ã‚¯ã‚¨ãƒªã«ã¨ã©ã‚ã¦ãã ã•ã„ã€‚
"""
    else:
        context = base_context

    # ===== ãƒ¡ã‚¤ãƒ³ã®è‡ªå¾‹æ¤œç´¢ãƒ«ãƒ¼ãƒ— =====
    for iteration in range(max_iterations):
        query_prompt = f"""{context}

ã€ã“ã‚Œã¾ã§ã®æ¤œç´¢å±¥æ­´ã€‘
{json.dumps(search_history, ensure_ascii=False, indent=2) if search_history else "ã¾ã æ¤œç´¢ã—ã¦ã„ã¾ã›ã‚“"}

ã€åé›†æ¸ˆã¿è¨˜äº‹æ•°ã€‘{len(all_items)}ä»¶

æ¬¡ã«å®Ÿè¡Œã™ã¹ãæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
- æ—¢å­˜ã®æ¤œç´¢ã¨é‡è¤‡ã—ãªã„æ–°ã—ã„åˆ‡ã‚Šå£ã§æ¢ã—ã¦ãã ã•ã„
- {lang}è¨€èªï¼ˆ{'æ—¥æœ¬èª' if lang == 'ja' else 'è‹±èª'}ï¼‰ã§ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
- å…·ä½“çš„ãªè£½å“åã€æŠ€è¡“åã€ç—‡ä¾‹ã‚¿ã‚¤ãƒ—ãªã©ã‚’å«ã‚ã¦ãã ã•ã„
- **æ¤œç´¢ã‚¯ã‚¨ãƒªã¯ã€Œæ–‡ç« ã€ã§ã¯ãªãæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³å‘ã‘ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã«ã—ã¦ãã ã•ã„**
  ï¼ˆä¾‹: æ—¥æœ¬èªãªã‚‰ "è‡ªå®¶æ­¯ç‰™ç§»æ¤ ç—‡ä¾‹",
       è‹±èªãªã‚‰ "tooth autotransplantation 3D printed replica"ï¼‰
- ã“ã‚Œã‚‰ã®ã‚¯ã‚¨ãƒªã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‚„è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆGoogle News / PubMed ãªã©ï¼‰ã§åˆ©ç”¨ã•ã‚Œã¾ã™

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
  "queries": [
    {{"query": "æ¤œç´¢ã‚¯ã‚¨ãƒª1", "reason": "ãªãœã“ã®æ¤œç´¢ãŒå¿…è¦ã‹"}},
    {{"query": "æ¤œç´¢ã‚¯ã‚¨ãƒª2", "reason": "ãªãœã“ã®æ¤œç´¢ãŒå¿…è¦ã‹"}},
    {{"query": "æ¤œç´¢ã‚¯ã‚¨ãƒª3", "reason": "ãªãœã“ã®æ¤œç´¢ãŒå¿…è¦ã‹"}}
  ],
  "strategy": "ä»Šå›ã®æ¤œç´¢æˆ¦ç•¥ã®èª¬æ˜"
}}

DO NOT OUTPUT ANYTHING OTHER THAN VALID JSON."""
        
        try:
            # OpenAI API ã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {OPENAI_API_KEY}"
                },
                json={
                    "model": "gpt-4o-mini",
                    "messages": [{"role": "user", "content": query_prompt}],
                    "max_tokens": 2000,
                    "temperature": 0.7
                },
                timeout=30
            )
            
            _d(f"[AI AGENT] API status code: {response.status_code}")
            
            if response.status_code != 200:
                _d(f"[AI AGENT] API error response: {response.text}")
                continue
            
            data = response.json()
            
            if "choices" not in data or len(data["choices"]) == 0:
                _d(f"[AI AGENT] Unexpected response format: {json.dumps(data, indent=2)}")
                continue
                
            result_text = data["choices"][0]["message"]["content"].strip()
            result_text = result_text.replace("```json", "").replace("```", "").strip()
            query_plan = json.loads(result_text)
            
            _d(f"[AI AGENT] Iteration {iteration+1}: {query_plan['strategy']}")
            
            for q_item in query_plan["queries"]:
                query = q_item["query"]
                reason = q_item["reason"]
                
                _d(f"[AI AGENT] Searching: {query}")
                
                search_results = []

                # â‘  Google News RSSï¼ˆ10-15ä»¶ç¨‹åº¦ï¼‰
                google_news_results = _execute_google_search(query, lang)
                search_results.extend(google_news_results)

                # â‘¡ Google Custom Search APIï¼ˆâ˜… åˆ¶é™ä»˜ãï¼‰
                if os.getenv("GOOGLE_API_KEY") and os.getenv("GOOGLE_CX_ID"):
                    if google_api_call_count < MAX_GOOGLE_API_CALLS:
                        google_api_results = _execute_google_search_api(query, lang, max_results=30)
                        search_results.extend(google_api_results)
                        google_api_call_count += 1
                    else:
                        _d(f"[Google Search API] Skipped - reached limit ({MAX_GOOGLE_API_CALLS} calls)")

                # â‘¢ PubMedï¼ˆè‹±èªã®ã¿ã€æœ€å¤§30ä»¶ï¼‰
                if lang == "en":
                    pubmed_results = _execute_pubmed_search(query, max_results=30)
                    search_results.extend(pubmed_results)

                # â‘£ YouTube RSSç‰ˆï¼ˆæœ€å¤§30ä»¶ï¼‰
                yt_rss_results = _execute_youtube_search(query, lang, max_results=30)
                search_results.extend(yt_rss_results)
                
                # â‘¤ YouTube Data APIç‰ˆï¼ˆæœ€å¤§30ä»¶ï¼‰
                if os.getenv("YOUTUBE_API_KEY"):
                    yt_api_results = _execute_youtube_search_api(query, lang, max_results=30)
                    search_results.extend(yt_api_results)
                
                # â˜… ã“ã“ã‹ã‚‰æ¤œç´¢çµæœã®å‡¦ç†ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã«æ³¨æ„ï¼ï¼‰
                for result_item in search_results:
                    if not result_item.get("url"):
                        continue
                    
                    # â˜… é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆè¿½åŠ ï¼‰
                    url = result_item["url"]
                    if any(pattern in url for pattern in EXCLUDED_PATTERNS):
                        _d(f"[FILTER] Skipped excluded URL: {url[:80]}...")
                        continue
                    
                    if url in collected_urls:
                        continue
                    
                    # â˜… Googleæ¤œç´¢çµæœã®å ´åˆã¯æœ¬æ–‡ã‚’å–å¾—ã—ã¦AIåˆ¤å®š
                    summary_for_ai = result_item.get("summary")
                    
                    if result_item.get("source") == "google_search_api":
                        _d(f"[AI] Google search result - fetching full content")
                        full_content = _fetch_content_for_ai(result_item["url"], max_chars=800)
                        if full_content:
                            _d(f"[AI] Fetched content ({len(full_content)} chars)")
                            summary_for_ai = full_content
                        else:
                            _d(f"[AI] Failed to fetch, using snippet")
                    
                    ai_result = ai_filter_and_classify(
                        result_item["title"], 
                        summary_for_ai,  # â† æœ¬æ–‡ã¾ãŸã¯ã‚¹ãƒ‹ãƒšãƒƒãƒˆ
                        lang,
                        result_item.get("url")
                    )
                    
                    if not ai_result["relevant"]:
                        continue

                    # åŸºæœ¬ã¯ AI ã® kind
                    kind = ai_result.get("kind", "research")

                    # YouTube ã‹ã‚‰æ¥ãŸã‚‚ã®ã¯å¿…ãš video æ‰±ã„
                    if result_item.get("source") in ["youtube", "youtube_api"]:
                        kind = "video"
                    
                    result_item["lang"] = lang
                    result_item["kind"] = kind
                    result_item["ai_relevant"] = ai_result["relevant"]
                    result_item["ai_kind"] = kind
                    result_item["ai_summary"] = ai_result["ai_summary"]
                    result_item["ai_reason"] = ai_result["reason"]
                    result_item["ai_search_query"] = query
                    result_item["ai_headline"] = ai_result.get("ai_headline")
                    
                    all_items.append(result_item)
                    collected_urls.add(result_item["url"])
                    
                    _d(
                        f"[AI AGENT] âœ“ Found: {result_item['title'][:60]}..."
                        f" (kind={kind}, lang={lang}, source={result_item.get('source')})"
                    )
                
                search_history.append({
                    "iteration": iteration + 1,
                    "query": query,
                    "reason": reason,
                    "found": len(search_results)
                })
                
                time.sleep(1)
                
        except Exception as e:
            _d(f"[AI AGENT] Error in iteration {iteration+1}: {e}")
            traceback.print_exc()

    # ===== ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ—¥æœ¬èªã§1ä»¶ã‚‚æ‹¾ãˆã¦ã„ãªã„å ´åˆ =====
    if lang == "ja" and not all_items:
        fallback_queries = [
            "è‡ªå®¶æ­¯ç‰™ç§»æ¤ ç—‡ä¾‹",
            "è‡ªå®¶æ­¯ç‰™ç§»æ¤ äºˆå¾Œ èª¿æŸ»",
            "æ­¯ã®è‡ªå®¶ç§»æ¤ ç ”ç©¶"
        ]
        for query in fallback_queries:
            _d(f"[AI AGENT] Fallback searching (ja): {query}")
            search_results = _execute_google_search(query, "ja")
            
            # Google Search APIç‰ˆã‚’è¿½åŠ ï¼ˆâ˜… åˆ¶é™ãªã— - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã®ã§ï¼‰
            if os.getenv("GOOGLE_API_KEY") and os.getenv("GOOGLE_CX_ID"):
                google_api = _execute_google_search_api(query, "ja", max_results=30)
                search_results.extend(google_api)
            
            # YouTube RSSç‰ˆã‚’è¿½åŠ 
            yt_rss = _execute_youtube_search(query, "ja", max_results=30)
            search_results.extend(yt_rss)
            
            # YouTube APIç‰ˆã‚’è¿½åŠ 
            if os.getenv("YOUTUBE_API_KEY"):
                yt_api = _execute_youtube_search_api(query, "ja", max_results=30)
                search_results.extend(yt_api)

            for result_item in search_results:
                if not result_item.get("url"):
                    continue
                
                # â˜… é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆè¿½åŠ ï¼‰
                url = result_item["url"]
                if any(pattern in url for pattern in EXCLUDED_PATTERNS):
                    _d(f"[FILTER] Skipped excluded URL: {url[:80]}...")
                    continue
                
                if result_item["url"] in collected_urls:
                    continue
                
                # â˜… Googleæ¤œç´¢çµæœã®å ´åˆã¯æœ¬æ–‡ã‚’å–å¾—ã—ã¦AIåˆ¤å®š
                summary_for_ai = result_item.get("summary")
                
                if result_item.get("source") == "google_search_api":
                    _d(f"[AI] Google search result - fetching full content")
                    full_content = _fetch_content_for_ai(result_item["url"], max_chars=800)
                    if full_content:
                        _d(f"[AI] Fetched content ({len(full_content)} chars)")
                        summary_for_ai = full_content
                    else:
                        _d(f"[AI] Failed to fetch, using snippet")
                
                ai_result = ai_filter_and_classify(
                    result_item["title"], 
                    summary_for_ai,  # â† æœ¬æ–‡ã¾ãŸã¯ã‚¹ãƒ‹ãƒšãƒƒãƒˆ
                    lang,
                    result_item.get("url")
                )
                
                if not ai_result["relevant"]:
                    continue

                # åŸºæœ¬ã¯ AI ã® kind
                kind = ai_result.get("kind", "research")

                # YouTubeï¼ˆRSSç‰ˆ/APIç‰ˆï¼‰ã‹ã‚‰æ¥ãŸã‚‚ã®ã¯å¿…ãš video æ‰±ã„ã«ä¸Šæ›¸ã
                if result_item.get("source") in ["youtube", "youtube_api"]:
                    kind = "video"
                
                result_item["lang"] = "ja"
                result_item["kind"] = kind
                result_item["ai_relevant"] = ai_result["relevant"]
                result_item["ai_kind"] = kind
                result_item["ai_summary"] = ai_result["ai_summary"]
                result_item["ai_reason"] = ai_result["reason"]
                result_item["ai_search_query"] = query
                result_item["ai_headline"] = ai_result.get("ai_headline")
                
                all_items.append(result_item)
                collected_urls.add(result_item["url"])
                
                _d(
                    f"[AI AGENT] âœ“ Found: {result_item['title'][:60]}..."
                    f" (kind={kind}, lang=ja, source={result_item.get('source')})"
                )

        # ãƒ­ã‚°ç”¨ã«å±¥æ­´ã‚‚è¿½åŠ ã—ã¦ãŠã
        search_history.append({
            "iteration": "fallback",
            "query": " / ".join(fallback_queries),
            "reason": "æ—¥æœ¬èªãƒ¢ãƒ¼ãƒ‰ã§0ä»¶ã ã£ãŸãŸã‚å›ºå®šã‚¯ã‚¨ãƒªã§å†æ¤œç´¢",
            "found": len(all_items)
        })
    
    # ===== DynamoDB ã¸ä¿å­˜ =====
    saved = 0
    for item in all_items:
        if put_unique_dental(item):
            saved += 1
            _d(f"[AI AGENT] ğŸ’¾ Saved: {item['title'][:60]}...")
    
    _d(f"[AI AGENT] âœ… Complete: total_found={len(all_items)}, saved={saved}")
    
    return {
        "total_found": len(all_items),
        "saved": saved,
        "search_history": search_history
    }


def _load_existing_urls_from_db():
    """DynamoDBã‹ã‚‰æ—¢å­˜ã®URLã‚’ã™ã¹ã¦å–å¾—ã—ã¦ã‚»ãƒƒãƒˆã§è¿”ã™ï¼ˆ2å›ç›®ä»¥é™ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰"""
    table = _table()
    existing_urls = set()
    
    scan_kwargs = {
        'ProjectionExpression': '#url',
        'FilterExpression': 'attribute_exists(#url)',
        'ExpressionAttributeNames': {
            '#url': 'url'
        }
    }
    
    try:
        while True:
            response = table.scan(**scan_kwargs)
            
            for item in response.get('Items', []):
                if 'url' in item:
                    existing_urls.add(item['url'])
            
            if 'LastEvaluatedKey' not in response:
                break
            scan_kwargs['ExclusiveStartKey'] = response['LastEvaluatedKey']
        
        _d(f"[CACHE] Loaded {len(existing_urls)} existing URLs from DynamoDB")
        
    except Exception as e:
        _d(f"[CACHE] Error loading existing URLs: {e}")
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œ
    
    return existing_urls


def _execute_google_search(query, lang="ja"):
    """å®Ÿéš›ã®Google Newsæ¤œç´¢ã‚’å®Ÿè¡Œ"""
    if lang == "ja":
        hl, gl, ceid = "ja", "JP", "JP:ja"
    else:
        hl, gl, ceid = "en", "US", "US:en"
    
    url = f"https://news.google.com/rss/search?q={quote_plus(query)}&hl={hl}&gl={gl}&ceid={ceid}"
    
    feed = feedparser.parse(url)
    items = []
    
    for e in feed.entries:
        link = getattr(e, "link", None)
        if not link:
            continue
            
        title = (getattr(e, "title", "") or "").strip()
        summary = getattr(e, "summary", None)
        
        pub = getattr(e, "published", None) or getattr(e, "updated", None)
        published_at = _iso_now_utc()
        if pub and getattr(e, "published_parsed", None):
            try:
                import datetime
                import time as _time
                tm = e.published_parsed
                published_at = datetime.datetime.utcfromtimestamp(
                    _time.mktime(tm)
                ).strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                pass
        
        items.append({
            "source": "google_news",  # âœ… ä¿®æ­£
            "title": title,
            "url": link,
            "published_at": published_at,
            "summary": summary,
            "author": getattr(getattr(e, "source", None) or {}, "title", None),
            "image_url": None,
            "lang": lang,
        })
    
    _d(f"[Google News RSS] Found {len(items)} articles for query: {query}")  # âœ… ãƒ­ã‚°è¿½åŠ 
    return items

def _execute_google_search_api(query, lang="ja", max_results=30):
    """Google Custom Search APIå®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
    api_key = os.getenv("GOOGLE_API_KEY")
    cx_id = os.getenv("GOOGLE_CX_ID")
    
    if not api_key or not cx_id:
        return []
    
    try:
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": cx_id,
            "q": query,
            "num": min(max_results, 10),
            "lr": f"lang_{lang}",
        }
        
        response = requests.get(url, params=params, timeout=10)
        
        # â˜… 429ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼‰ã®å‡¦ç†
        if response.status_code == 429:
            _d(f"[Google Search API] Rate limit reached - skipping this query")
            return []
        
        if response.status_code != 200:
            _d(f"[Google Search API] Error: {response.status_code}")
            return []
        
        data = response.json()
        items = []
        
        for item in data.get("items", []):
            items.append({
                "source": "google_search_api",
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "published_at": _iso_now_utc(),
                "summary": item.get("snippet", ""),
                "author": None,
                "image_url": item.get("pagemap", {}).get("cse_image", [{}])[0].get("src"),
                "lang": lang,
            })
        
        _d(f"[Google Search API] Found {len(items)} articles")
        return items
        
    except Exception as e:
        _d(f"[Google Search API] Error: {e}")
        return []

def _execute_youtube_search(query, lang="ja", max_results=20):
    """
    YouTube æ¤œç´¢ï¼ˆRSSï¼‰ã‹ã‚‰å‹•ç”»ä¸€è¦§ã‚’å–å¾—
    è¿”ã‚Šå€¤ã¯ä»–ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼š
    {source, title, url, published_at, summary, author, image_url, lang}
    """
    # YouTube æ¤œç´¢ç”¨ RSS ãƒ•ã‚£ãƒ¼ãƒ‰
    # ä¾‹: https://www.youtube.com/feeds/videos.xml?search_query=%E8%87%AA%E5%AE%B6%E6%AD%AF%E7%89%99%E7%A7%BB%E6%A4%8D
    url = f"https://www.youtube.com/feeds/videos.xml?search_query={quote_plus(query)}"

    feed = feedparser.parse(url)
    items = []

    for e in feed.entries[:max_results]:
        link = getattr(e, "link", None)
        if not link:
            continue

        title = (getattr(e, "title", "") or "").strip()
        summary = getattr(e, "summary", None)

        # æŠ•ç¨¿æ—¥
        pub = getattr(e, "published", None) or getattr(e, "updated", None)
        published_at = _iso_now_utc()
        if pub and getattr(e, "published_parsed", None):
            try:
                import datetime
                import time as _time
                tm = e.published_parsed
                published_at = datetime.datetime.utcfromtimestamp(
                    _time.mktime(tm)
                ).strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                pass

        # ãƒãƒ£ãƒ³ãƒãƒ«å
        author = getattr(e, "author", None)

        # ã‚µãƒ ãƒã‚¤ãƒ«ï¼ˆã‚ã‚Œã°ï¼‰
        thumb = None
        if "media_thumbnail" in e:
            try:
                thumb = e.media_thumbnail[0]["url"]
            except Exception:
                pass

        items.append({
            "source": "youtube",
            "title": title,
            "url": link,
            "published_at": published_at,
            "summary": summary,
            "author": author,
            "image_url": thumb,
            "lang": lang,
            # kind ã¯å¾Œã§å¼·åˆ¶çš„ã« "video" ã«ã™ã‚‹
        })

    return items

def _execute_youtube_search_api(query: str, lang: str = "ja", max_results: int = 10) -> list:
    """YouTube Data APIç‰ˆï¼ˆã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢ãƒ»APIã‚­ãƒ¼å¿…è¦ï¼‰"""
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        _d("[YouTube API] API key not found, skipping")
        return []
    
    try:
        url = "https://www.googleapis.com/youtube/v3/search"
        params = {
            "part": "snippet",
            "q": query,
            "type": "video",
            "maxResults": max_results,
            "key": api_key,
            "relevanceLanguage": lang,
            "order": "date",  # æœ€æ–°é †
            "regionCode": "JP" if lang == "ja" else "US"
        }
        
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code != 200:
            _d(f"[YouTube API] Error: {response.status_code} - {response.text}")
            return []
        
        data = response.json()
        items = []
        
        for item in data.get("items", []):
            video_id = item["id"].get("videoId")
            if not video_id:
                continue
            
            snippet = item["snippet"]
            
            # å…¬é–‹æ—¥æ™‚ã‚’ISOå½¢å¼ã«å¤‰æ›
            published_at = snippet.get("publishedAt", "")
            
            items.append({
                "source": "youtube_api",
                "title": snippet.get("title", ""),
                "url": f"https://www.youtube.com/watch?v={video_id}",
                "published_at": published_at,
                "summary": snippet.get("description", "")[:200],
                "author": snippet.get("channelTitle", ""),
                "image_url": snippet.get("thumbnails", {}).get("high", {}).get("url", ""),
                "lang": lang,
            })
        
        _d(f"[YouTube API] Found {len(items)} videos for query: {query}")
        return items
        
    except Exception as e:
        _d(f"[YouTube API] Error: {e}")
        traceback.print_exc()
        return []


def _execute_pubmed_search(query: str, max_results: int = 20):
    """PubMed ã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—ã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã™"""
    try:
        # 1. ID ãƒªã‚¹ãƒˆã‚’å–å¾—
        r = requests.get(
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
            params={
                "db": "pubmed",
                "term": query,
                "retmode": "json",
                "retmax": max_results,
                "sort": "pub+date",   # ç™ºè¡Œæ—¥ã®æ–°ã—ã„é †
            },
            timeout=10,
        )
        r.raise_for_status()
        data = r.json()
        ids = data.get("esearchresult", {}).get("idlist", [])
        if not ids:
            return []

        id_str = ",".join(ids)

        # 2. è©³ç´°æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãªã©ï¼‰ã‚’ XML ã§å–å¾—
        r2 = requests.get(
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
            params={
                "db": "pubmed",
                "id": id_str,
                "retmode": "xml",
            },
            timeout=20,
        )
        r2.raise_for_status()
        root = ET.fromstring(r2.text)

        def _parse_pubdate(pubdate_elem):
            """PubDate è¦ç´ ã‹ã‚‰ ISO æ–‡å­—åˆ—ã‚’ã§ãã‚‹ç¯„å›²ã§ä½œã‚‹"""
            if pubdate_elem is None:
                return _iso_now_utc()

            year = pubdate_elem.findtext("Year")
            month = pubdate_elem.findtext("Month") or "01"
            day = pubdate_elem.findtext("Day") or "01"

            # æœˆãŒ "Jan" ãªã©ã®çœç•¥è¡¨è¨˜ã®å ´åˆã«å¯¾å¿œ
            month_map = {
                "Jan": "01", "Feb": "02", "Mar": "03", "Apr": "04",
                "May": "05", "Jun": "06", "Jul": "07", "Aug": "08",
                "Sep": "09", "Oct": "10", "Nov": "11", "Dec": "12",
            }
            month = month_map.get(month, month)

            try:
                dt = datetime(int(year), int(month), int(day))
                return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                return _iso_now_utc()

        items = []

        for art in root.findall(".//PubmedArticle"):
            pmid_el = art.find(".//PMID")
            pmid = pmid_el.text if pmid_el is not None else None

            article = art.find(".//Article")
            title = ""
            if article is not None:
                title_el = article.find("ArticleTitle")
                if title_el is not None:
                    # ã‚¿ã‚°ã‚’å«ã‚€ã“ã¨ãŒã‚ã‚‹ã®ã§ itertext ã§çµåˆ
                    title = "".join(title_el.itertext()).strip()

            abstract = ""
            abstr_el = article.find("Abstract") if article is not None else None
            if abstr_el is not None:
                parts = []
                for t in abstr_el.findall("AbstractText"):
                    parts.append("".join(t.itertext()).strip())
                abstract = " ".join(parts)

            journal = ""
            journal_el = article.find("Journal/Title") if article is not None else None
            if journal_el is not None:
                journal = journal_el.text

            pubdate_el = art.find(".//PubDate")
            published_at = _parse_pubdate(pubdate_elem=pubdate_el)

            url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else None

            items.append({
                "source": "pubmed",
                "pmid": pmid,
                "title": title,
                "url": url,
                "published_at": published_at,
                "summary": abstract,
                "author": journal,   # or first author ã§ã‚‚OK
                "image_url": None,
                "lang": "en",        # PubMed ã¯åŸºæœ¬è‹±èªæ‰±ã„
            })

        return items

    except Exception as e:
        _d(f"[PUBMED] Error: {e}")
        traceback.print_exc()
        return []

# ========= ã‚µãƒ¼ãƒ“ã‚¹ =========
def _enc_tok(lek: dict | None) -> str | None:
    if not lek: return None
    return base64.urlsafe_b64encode(json.dumps(lek).encode()).decode()

def _dec_tok(tok: str | None):
    if not tok: return None
    try:
        return json.loads(base64.urlsafe_b64decode(tok.encode()).decode())
    except Exception:
        return None

def list_news(kind="research", lang="ja", limit=40, tok=None):
    lek = _dec_tok(tok)
    items, next_lek = dental_query_items(kind=kind, lang=lang, limit=limit, last_evaluated_key=lek)
    return items, _enc_tok(next_lek)

# ========= ãƒ«ãƒ¼ãƒˆ =========
@bp.route("/news")
def news():
    return render_template("pages/news.html")

@bp.route("/autotransplant_news")
def autotransplant_news():
    kind = request.args.get("kind", "research")
    lang = request.args.get("lang", "ja")
    tok  = request.args.get("tok")
    rows, next_tok = list_news(kind=kind, lang=lang, limit=40, tok=tok)
    return render_template("pages/autotransplant_news.html",
                           rows=rows, kind=kind, lang=lang, page=1, next_tok=next_tok)

@bp.route("/api/latest")
def news_api_latest():
    kind = request.args.get("kind", "research")
    lang = request.args.get("lang", "ja")
    limit = min(int(request.args.get("limit", 5)), 20)

    # â˜… lang=all ã®ã¨ãã¯ ja + en ã‚’ã¾ã¨ã‚ã¦è¿”ã™
    if lang == "all":
        combined = []
        for lg in ["ja", "en"]:
            items, _ = dental_query_items(kind=kind, lang=lg, limit=limit)
            combined.extend(items)

        # published_at ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        combined.sort(key=lambda x: x.get("published_at", ""), reverse=True)

        # URLã§é‡è¤‡æ’é™¤ã—ã¤ã¤ã€æœ€å¤§ limit ä»¶ã¾ã§
        seen = set()
        payload = []
        for it in combined:
            url = it.get("url")
            if not url or url in seen:
                continue
            seen.add(url)

            # â˜… è¦‹å‡ºã—ã®å„ªå…ˆé †ä½: ai_headline > title
            headline = it.get("ai_headline") or it.get("title")

            payload.append({
                "title": headline,
                "url": url,
                "published_at": (it.get("published_at") or "")[:10],
                "kind": it.get("kind"),
                "lang": it.get("lang"),
                "source": it.get("source"),
                "ai_headline": it.get("ai_headline"),  # â† è¿½åŠ 
                "ai_summary": it.get("ai_summary"),    # â† è¿½åŠ 
            })

            if len(payload) >= limit:
                break

        return jsonify({
            "kind": kind, "lang": "all",
            "count": len(payload),
            "updated_at": _iso_now_utc(),
            "items": payload,
        })

    # â˜… lang ãŒ ja / en ã®ã¨ãï¼ˆä¿®æ­£ç‰ˆï¼‰
    items, _ = dental_query_items(kind=kind, lang=lang, limit=limit, last_evaluated_key=None)
    
    payload = []
    for it in items:
        if not it.get("title") or not it.get("url"):
            continue
            
        # â˜… è¦‹å‡ºã—ã®å„ªå…ˆé †ä½: ai_headline > title
        headline = it.get("ai_headline") or it.get("title")
        
        payload.append({
            "title": headline,
            "url": it.get("url"),
            "published_at": (it.get("published_at") or "")[:10],
            "kind": it.get("kind"),
            "ai_headline": it.get("ai_headline"),  # â† è¿½åŠ 
            "ai_summary": it.get("ai_summary"),    # â† è¿½åŠ 
        })
    
    return jsonify({
        "kind": kind, "lang": lang,
        "count": len(payload),
        "updated_at": _iso_now_utc(),
        "items": payload
    })


@bp.route("/api/all")
def news_api_all():
    """å…¨ã¦ã®è¨˜äº‹ã‚’å–å¾—ï¼ˆå…¨ç¨®é¡ãƒ»å…¨è¨€èªï¼‰"""
    all_items = []
    
    # å…¨ç¨®é¡ãƒ»å…¨è¨€èªã‚’å–å¾—
    kinds = ["research", "case", "news", "video", "product", "market"]  # â† "news" ã‚’è¿½åŠ 
    langs = ["ja", "en"]
    
    for kind in kinds:
        for lang in langs:
            items, _ = dental_query_items(kind=kind, lang=lang, limit=100)
            all_items.extend(items)
    
    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    all_items.sort(key=lambda x: x.get("published_at", ""), reverse=True)
    
    # é‡è¤‡å‰Šé™¤ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
    seen_urls = set()
    unique_items = []
    for item in all_items:
        if item["url"] not in seen_urls:
            seen_urls.add(item["url"])
            unique_items.append({
                "title": item.get("title"),
                "url": item.get("url"),
                "published_at": (item.get("published_at") or "")[:10],
                "kind": item.get("kind"),
                "lang": item.get("lang"),
                "ai_summary": item.get("ai_summary"),
                "author": item.get("author"),
                "image_url": item.get("image_url"),
            })
    
    return jsonify({
        "count": len(unique_items),
        "updated_at": _iso_now_utc(),
        "items": unique_items
    })


@bp.route("/admin/inspect")
def news_admin_inspect():
    kinds = ["research", "case", "video", "product", "market"]
    langs = ["ja", "en"]
    lines = []
    for k in kinds:
        for lg in langs:
            items, _ = dental_query_items(kind=k, lang=lg, limit=5)
            titles = [i.get("title") for i in items]
            lines.append(f"{k}/{lg}: count~{len(items)} sample={titles}")
    _d("[INSPECT] " + " | ".join(lines))
    return "<br>".join(lines)

@bp.route("/admin/debug_dump")
def news_admin_debug_dump():
    items, _ = dental_query_items(kind="research", lang="ja", limit=20)
    html = ["<h3>research/ja (top 20)</h3><ol>"]
    for it in items:
        html.append(f"<li>{it.get('published_at','')} â€” {it.get('title','')}<br>"
                    f"<small>{it.get('url','')}</small></li>")
    html.append("</ol>")
    return "".join(html)


@bp.route("/admin/run_autotransplant_news")
def run_autotransplant_news():
    """AIåé›†ã‚’å®Ÿè¡Œï¼ˆæ—¢å­˜ã®ãƒ«ãƒ¼ãƒˆã‹ã‚‰å‘¼ã³å‡ºã—ï¼‰"""
    try:
        # AIåé›†ã‚’å®Ÿè¡Œ
        results_ja = ai_collect_news(lang="ja", max_iterations=5)
        time.sleep(2)
        results_en = ai_collect_news(lang="en", max_iterations=3)
        
        total = results_ja["saved"] + results_en["saved"]
        
        # æœ€åˆã®è¨˜äº‹ã‚’ç¢ºèª
        items, _ = dental_query_items(kind="research", lang="ja", limit=1)
        
        _d(f"[DEBUG] after collect: total={total}, ja={results_ja['saved']}, en={results_en['saved']}")
        
        return f"""
        <h1>AIåé›†å®Œäº†ï¼</h1>
        <p>åˆè¨ˆ: {total}ä»¶ã®è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ</p>
        <ul>
            <li>æ—¥æœ¬èª: {results_ja['saved']}ä»¶ï¼ˆæ¤œç´¢{len(results_ja['search_history'])}å›ï¼‰</li>
            <li>è‹±èª: {results_en['saved']}ä»¶ï¼ˆæ¤œç´¢{len(results_en['search_history'])}å›ï¼‰</li>
        </ul>
        <p>æœ€åˆã®è¨˜äº‹: {'ã‚ã‚Š' if items else 'ãªã—'}</p>
        <h3>æ¤œç´¢å±¥æ­´ï¼ˆæ—¥æœ¬èªï¼‰:</h3>
        <pre>{json.dumps(results_ja['search_history'], ensure_ascii=False, indent=2)}</pre>
        <h3>æ¤œç´¢å±¥æ­´ï¼ˆè‹±èªï¼‰:</h3>
        <pre>{json.dumps(results_en['search_history'], ensure_ascii=False, indent=2)}</pre>
        <p><a href="/news/autotransplant_news?kind=research&lang=ja">æ—¥æœ¬èªè¨˜äº‹ã‚’è¦‹ã‚‹</a></p>
        <p><a href="/news/autotransplant_news?kind=research&lang=en">è‹±èªè¨˜äº‹ã‚’è¦‹ã‚‹</a></p>
        """
    except Exception as e:
        traceback.print_exc()
        return f"<h1>ã‚¨ãƒ©ãƒ¼</h1><pre>{traceback.format_exc()}</pre>"
    

@bp.route("/admin/clear_all_dental_news")
def clear_all_dental_news():
    """å…¨è¨˜äº‹ã‚’å®Œå…¨å‰Šé™¤"""
    try:
        table = current_app.config["DENTAL_TABLE"]
        deleted = 0
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        scan_kwargs = {
            'ProjectionExpression': 'pk, sk'
        }
        
        while True:
            response = table.scan(**scan_kwargs)
            items = response.get('Items', [])
            
            # ãƒãƒƒãƒã§å‰Šé™¤
            with table.batch_writer() as batch:
                for item in items:
                    batch.delete_item(Key={
                        'pk': item['pk'],
                        'sk': item['sk']
                    })
                    deleted += 1
            
            if 'LastEvaluatedKey' not in response:
                break
            scan_kwargs['ExclusiveStartKey'] = response['LastEvaluatedKey']
        
        return jsonify({
            "status": "success",
            "message": f"å‰Šé™¤å®Œäº†: {deleted}ä»¶"
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }), 500
    
@bp.route("/admin/count_dental_news")
def count_dental_news():
    """è¨˜äº‹ã®ç·æ•°ã‚’ç¢ºèª"""
    try:
        table = current_app.config["DENTAL_TABLE"]
        
        response = table.scan(Select='COUNT')
        count = response.get('Count', 0)
        
        while 'LastEvaluatedKey' in response:
            response = table.scan(
                Select='COUNT',
                ExclusiveStartKey=response['LastEvaluatedKey']
            )
            count += response.get('Count', 0)
        
        return jsonify({
            "status": "success",
            "table": os.getenv("DENTAL_TABLE_NAME", "dental-news"),
            "total_count": count
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500
    

def dental_query_items(kind=None, lang=None, limit=40, last_evaluated_key=None):
    table = current_app.config["DENTAL_TABLE"]

    if not kind:
        kind = "research"
    if not lang:
        lang = "ja"

    pk = f"KIND#{kind}#LANG#{lang}"

    # â˜… scan ã§ã¯ãªã query ã‚’ä½¿ã†ï¼ˆGSI1ã‚’åˆ©ç”¨ï¼‰
    query_kwargs = {
        "IndexName": "gsi1",  # GSIã®åå‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„
        "KeyConditionExpression": "gsi1pk = :pk",
        "ExpressionAttributeValues": {
            ":pk": pk
        },
        "ScanIndexForward": False,  # æ–°ã—ã„é †ï¼ˆgsi1skã§é™é †ï¼‰
        "Limit": limit,
    }

    if last_evaluated_key:
        query_kwargs["ExclusiveStartKey"] = last_evaluated_key

    resp = table.query(**query_kwargs)
    items = resp.get("Items", [])

    return items, resp.get("LastEvaluatedKey")


def put_unique_dental(item: dict) -> bool:
    """æ­¯ç§‘ãƒ‹ãƒ¥ãƒ¼ã‚¹å°‚ç”¨ã®DynamoDBä¿å­˜é–¢æ•°ï¼ˆAIå¯¾å¿œç‰ˆï¼‰"""
    pk = f"URL#{_hash_url(item['url'])}"
    try:
        _table().put_item(
            Item=_dynamodb_sanitize({
                "pk": pk, "sk": "METADATA",
                "url": item["url"],
                "title": item.get("title"),
                "source": item.get("source"),
                "kind": item.get("kind"),
                "lang": item.get("lang"),
                "published_at": (_ensure_iso(item.get("published_at")) or _iso_now_utc()),
                "summary": item.get("summary"),
                "image_url": item.get("image_url"),
                "author": item.get("author"),
                "gsi1pk": f"KIND#{item.get('kind')}#LANG#{item.get('lang')}",
                "gsi1sk": _ensure_iso(item.get("published_at")) or "0000-00-00T00:00:00",
                # AI åˆ¤å®šçµæœ
                "ai_relevant": item.get("ai_relevant"),
                "ai_kind": item.get("ai_kind"),
                "ai_summary": item.get("ai_summary"),
                "ai_reason": item.get("ai_reason"),
                "ai_search_query": item.get("ai_search_query"),
                "ai_headline": item.get("ai_headline"),
            }),
            ConditionExpression="attribute_not_exists(pk)",
        )
        _d(f"[DB] âœ… Saved: {item.get('title','')[:50]}")
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "ConditionalCheckFailedException":
            _d(f"[DB] Skipped (already exists): {item.get('title','')[:50]}")  # â† ã‚¿ã‚¤ãƒˆãƒ«è¿½åŠ 
            return False
        _d(f"[DB] Error in put_unique_dental: {e}")
        return False
    

def _fetch_content_for_ai(url: str, max_chars: int = 500):
    """
    URLã‹ã‚‰æœ¬æ–‡ã‚’å–å¾—ã—ã¦AIåˆ¤å®šç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    å–å¾—ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã™
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ä¸è¦ãªã‚¿ã‚°ã‚’å‰Šé™¤
        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
            tag.decompose()
        
        # æœ¬æ–‡ã‚’æŠ½å‡º
        text = soup.get_text(separator=' ', strip=True)
        
        # ç©ºç™½ã‚’æ­£è¦åŒ–
        text = ' '.join(text.split())
        
        # æŒ‡å®šæ–‡å­—æ•°ã¾ã§åˆ‡ã‚Šå–ã‚Š
        return text[:max_chars] if text else None
        
    except Exception as e:
        _d(f"[FETCH CONTENT] Error fetching {url}: {e}")
        return None
    
# 2. æœ€è¿‘ã®æ—¥æœ¬èªè¨˜äº‹ã‚’å…¨éƒ¨è¡¨ç¤º
def list_recent_ja_articles(limit=50):
    table = _table()
    response = table.query(
        IndexName="gsi1",
        KeyConditionExpression="gsi1pk = :pk",
        ExpressionAttributeValues={
            ":pk": "KIND#case#LANG#ja"  # ã¾ãŸã¯ "KIND#research#LANG#ja"
        },
        ScanIndexForward=False,  # æ–°ã—ã„é †
        Limit=limit
    )
    
    for item in response['Items']:
        print(f"{item.get('published_at')} - {item.get('title')}")


@bp.route('/debug_china')
def debug_china():
    """dental-plazaã®è¨˜äº‹ã‚’ç¢ºèª"""
    table = _table()
    
    response = table.scan()
    
    dental_plaza_items = []
    for item in response['Items']:
        url = item.get('url', '')
        if 'dental-plaza' in url or 'åˆ‡æ­¯éª¨' in item.get('title', ''):
            dental_plaza_items.append(item)
    
    html = f"<h1>Dental Plaza / åˆ‡æ­¯éª¨ Articles ({len(dental_plaza_items)}ä»¶)</h1>"
    
    for item in dental_plaza_items:
        html += f"""
        <div style="margin-bottom: 20px; border: 1px solid #ccc; padding: 10px;">
            <h3>{item.get('title')}</h3>
            <p><strong>URL:</strong> <a href="{item.get('url')}" target="_blank">{item.get('url')}</a></p>
            <p><strong>Kind:</strong> {item.get('kind')} | <strong>Lang:</strong> {item.get('lang')}</p>
            <p><strong>Published:</strong> {item.get('published_at')}</p>
            <p><strong>Search Query:</strong> {item.get('ai_search_query')}</p>
            <p><strong>AI Reason:</strong> {item.get('ai_reason')}</p>
        </div>
        """
    
    if not dental_plaza_items:
        html += "<p>è©²å½“ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</p>"
    
    return html
    
@bp.route('/debug_article')
def debug_article():
    """ç‰¹å®šè¨˜äº‹ã®AIåˆ¤å®šã‚’ç¢ºèª"""
    article_url = request.args.get('url')
    
    if not article_url:
        return "URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚ä¾‹: /news/debug_article?url=https://...", 400
    
    table = _table()
    pk = f"URL#{_hash_url(article_url)}"
    
    try:
        response = table.get_item(Key={"pk": pk, "sk": "METADATA"})
        item = response.get('Item')
        
        if item:
            html = f"""
            <h1>AIåˆ¤å®šçµæœ</h1>
            <h2>åŸºæœ¬æƒ…å ±</h2>
            <p><strong>Title:</strong> {item.get('title')}</p>
            <p><strong>URL:</strong> <a href="{item.get('url')}" target="_blank">{item.get('url')}</a></p>
            <p><strong>Source:</strong> {item.get('source')}</p>
            <p><strong>Kind:</strong> {item.get('kind')}</p>
            <p><strong>Lang:</strong> {item.get('lang')}</p>
            <p><strong>Published:</strong> {item.get('published_at')}</p>
            
            <h2>åé›†æ™‚ã®è¦ç´„</h2>
            <p><strong>Summary:</strong> {item.get('summary')}</p>
            
            <h2>AIåˆ¤å®š</h2>
            <p><strong>AI Relevant:</strong> {item.get('ai_relevant')}</p>
            <p><strong>AI Kind:</strong> {item.get('ai_kind')}</p>
            <p><strong>AI Reason:</strong> {item.get('ai_reason')}</p>
            <p><strong>AI Summary:</strong> {item.get('ai_summary')}</p>
            <p><strong>Search Query:</strong> {item.get('ai_search_query')}</p>
            """
            return html
        else:
            return f"è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {article_url}", 404
            
    except Exception as e:
        import traceback
        return f"<pre>{traceback.format_exc()}</pre>", 500
    
@bp.route('/debug_dental_plaza')
def debug_dental_plaza():
    """dental-plazaã®è¨˜äº‹ã‚’å…¨ã¦ãƒªã‚¹ãƒˆ"""
    table = _table()
    
    response = table.scan()
    
    dental_plaza_items = []
    for item in response['Items']:
        url = item.get('url', '')
        if 'dental-plaza' in url:
            dental_plaza_items.append(item)
    
    html = f"<h1>Dental Plaza Articles ({len(dental_plaza_items)}ä»¶)</h1>"
    
    for item in dental_plaza_items:
        html += f"""
        <div style="margin-bottom: 20px; border: 1px solid #ccc; padding: 10px;">
            <h3>{item.get('title')}</h3>
            <p><strong>URL:</strong> <a href="{item.get('url')}" target="_blank">{item.get('url')}</a></p>
            <p><strong>Kind:</strong> {item.get('kind')} | <strong>Lang:</strong> {item.get('lang')}</p>
            <p><strong>Published:</strong> {item.get('published_at')}</p>
            <p><strong>Search Query:</strong> {item.get('ai_search_query')}</p>
            <p><strong>AI Reason:</strong> {item.get('ai_reason')}</p>
            <hr>
            <p><a href="/news/debug_article?url={item.get('url')}" target="_blank">è©³ç´°ã‚’è¦‹ã‚‹</a></p>
        </div>
        """
    
    return html

@bp.route("/admin/check_chinese_article")
def check_chinese_article():
    """ä¸­å›½è¨˜äº‹ã®å­˜åœ¨ç¢ºèª"""
    table = current_app.config["DENTAL_TABLE"]
    
    # å…¨è¨˜äº‹ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ä¸­å›½é–¢é€£ã‚’æ¤œç´¢
    response = table.scan(
        FilterExpression="contains(title, :keyword)",
        ExpressionAttributeValues={
            ':keyword': 'ä¸­å›½'
        }
    )
    
    return jsonify({
        "found": len(response.get('Items', [])),
        "items": response.get('Items', [])
    })


@bp.route("/admin/count_by_kind")
def count_by_kind():
    """ç¨®é¡åˆ¥ã®è¨˜äº‹æ•°ã‚’ç¢ºèª"""
    table = current_app.config["DENTAL_TABLE"]
    
    kind_counts = {}
    langs = ["ja", "en"]
    
    for lang in langs:
        for kind in ["research", "case", "news", "video", "product", "market"]:
            items, count = dental_query_items(kind=kind, lang=lang, limit=1000)
            key = f"{kind}_{lang}"
            kind_counts[key] = len(items)
    
    # å…¨ä½“ã®çµ±è¨ˆ
    total_ja = sum(v for k, v in kind_counts.items() if k.endswith('_ja'))
    total_en = sum(v for k, v in kind_counts.items() if k.endswith('_en'))
    
    return jsonify({
        "by_kind_and_lang": kind_counts,
        "summary": {
            "total_ja": total_ja,
            "total_en": total_en,
            "total": total_ja + total_en
        },
        "news_ja": kind_counts.get("news_ja", 0),
        "news_en": kind_counts.get("news_en", 0),
        "total_news": kind_counts.get("news_ja", 0) + kind_counts.get("news_en", 0)
    })
